{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Lecture 5: Preprocessing and `sklearn` pipelines\n",
    "\n",
    "UBC 2020-21\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "# pip install git+git://github.com/mgelbart/plot-classifier.git\n",
    "from plot_classifier import plot_classifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Classifiers and regressors\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "\n",
    "# Preprocessing and pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# train test split and cross validation\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lecture learning objectives\n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- identify when to implement feature transformations such as imputation, scaling, and one-hot encoding in a machine learning model development pipeline; \n",
    "- use `sklearn` for applying feature transformations on your dataset;\n",
    "- discuss golden rule in the context of feature transformations;\n",
    "- use `sklearn.pipeline.Pipeline` to build a preliminary machine learning pipeline; \n",
    "- use `ColumnTransformer` to build all our transformations together into one object and use it with `sklearn` pipelines.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Lecture outline\n",
    "\n",
    "1. [Introduction and motivation](#1)\n",
    "2. [Dataset and baseline](#2)\n",
    "2. [Imputation](#3)\n",
    "3. [Scaling](#4)\n",
    "3. [Feature transformations and the golden rule](#5)\n",
    "5. [Categorical variables](#6)\n",
    "6. [ColumnTransformer](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation and big picture <a name=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- So far we have seen\n",
    "    - Three ML models (decision trees, $k$-NNs, SVMs with RBF kernel)\n",
    "    - ML fundamentals (train-validation-test split, cross-validation, the fundamental tradeoff, the golden rule)\n",
    "- Are we ready to do machine learning on real-world datasets?\n",
    "    - Very often real-world datasets need preprocessing before we use them to build ML models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: $k$-nearest neighbours on the Spotify dataset\n",
    "\n",
    "- In lab1 you used `DecisionTreeClassifier` to predict whether the user would like a particular song or not. \n",
    "- Can we use $k$-NN classifier for this task? \n",
    "- Intuition: To predict whether the user likes a particular song or not (query point) \n",
    "   - find the songs that are closest to the query point\n",
    "   - let them vote on the target\n",
    "   - take the majority vote as the target for the query point\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "spotify_df = pd.read_csv(\"data/spotify.csv\", index_col=0)\n",
    "train_df, test_df = train_test_split(spotify_df, test_size=0.20, random_state=123)\n",
    "X_train, y_train = (\n",
    "    train_df.drop(columns=[\"song_title\", \"artist\", \"target\"]),\n",
    "    train_df[\"target\"],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    test_df.drop(columns=[\"song_title\", \"artist\", \"target\"]),\n",
    "    test_df[\"target\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation score 0.508\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.507740</td>\n",
       "      <td>0.507752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.507740</td>\n",
       "      <td>0.507752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.507740</td>\n",
       "      <td>0.507752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.506211</td>\n",
       "      <td>0.508133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.507359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.000900    0.000288    0.507740     0.507752\n",
       "1  0.000488    0.000202    0.507740     0.507752\n",
       "2  0.000468    0.000206    0.507740     0.507752\n",
       "3  0.000471    0.000201    0.506211     0.508133\n",
       "4  0.000466    0.000201    0.509317     0.507359"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "scores = cross_validate(dummy, X_train, y_train, return_train_score=True)\n",
    "print(\"Mean validation score %0.3f\" % (np.mean(scores[\"test_score\"])))\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean validation score 0.546\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.563467</td>\n",
       "      <td>0.717829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.535604</td>\n",
       "      <td>0.721705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.007331</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.708527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.007426</td>\n",
       "      <td>0.537267</td>\n",
       "      <td>0.721921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.007635</td>\n",
       "      <td>0.562112</td>\n",
       "      <td>0.711077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.002285    0.008381    0.563467     0.717829\n",
       "1  0.001910    0.007422    0.535604     0.721705\n",
       "2  0.001753    0.007331    0.529412     0.708527\n",
       "3  0.001867    0.007426    0.537267     0.721921\n",
       "4  0.001843    0.007635    0.562112     0.711077"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "scores = cross_validate(knn, X_train, y_train, return_train_score=True)\n",
    "print(\"Mean validation score %0.3f\" % (np.mean(scores[\"test_score\"])))\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.494</td>\n",
       "      <td>147893</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>-9.743</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>140.832</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.771</td>\n",
       "      <td>227143</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>-4.712</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>111.959</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     acousticness  danceability  duration_ms  energy  instrumentalness  key  \\\n",
       "842      0.229000         0.494       147893   0.666          0.000057    9   \n",
       "654      0.000289         0.771       227143   0.949          0.602000    8   \n",
       "\n",
       "     liveness  loudness  mode  speechiness    tempo  time_signature  valence  \n",
       "842    0.0469    -9.743     0       0.0351  140.832             4.0    0.704  \n",
       "654    0.5950    -4.712     1       0.1750  111.959             4.0    0.372  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_songs = X_train.sample(2, random_state=42)\n",
    "two_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.        , 79250.00543825],\n",
       "       [79250.00543825,     0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(two_songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider only two features: `duration_ms` and `tempo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>tempo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>147893</td>\n",
       "      <td>140.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>227143</td>\n",
       "      <td>111.959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     duration_ms    tempo\n",
       "842       147893  140.832\n",
       "654       227143  111.959"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_songs_subset = two_songs[[\"duration_ms\", \"tempo\"]]\n",
    "two_songs_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0.        , 79250.00525962],\n",
       "       [79250.00525962,     0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances(two_songs_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The distance is completely dominated by the the features with larger values\n",
    "- The features with smaller values are being ignored. \n",
    "- Does it matter? \n",
    "    - Yes! Scale is based on how data was collected. \n",
    "    - Features on a smaller scale can be highly informative and there is no good reason to ignore them.\n",
    "    - We want our model to be robust and not sensitive to the scale. \n",
    "- Was this a problem for decision trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scaling using `scikit-learn`'s [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "- We'll use `scikit-learn`'s [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), which is a `transformer`.   \n",
    "- Only focus on the syntax for now. We'll talk about scaling in a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.697633</td>\n",
       "      <td>-0.194548</td>\n",
       "      <td>-0.398940</td>\n",
       "      <td>-0.318116</td>\n",
       "      <td>-0.492359</td>\n",
       "      <td>1.275623</td>\n",
       "      <td>-0.737898</td>\n",
       "      <td>0.395794</td>\n",
       "      <td>-1.280599</td>\n",
       "      <td>-0.617752</td>\n",
       "      <td>-0.293827</td>\n",
       "      <td>0.138514</td>\n",
       "      <td>-0.908149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.276291</td>\n",
       "      <td>0.295726</td>\n",
       "      <td>-0.374443</td>\n",
       "      <td>-0.795552</td>\n",
       "      <td>0.598355</td>\n",
       "      <td>-1.487342</td>\n",
       "      <td>-0.438792</td>\n",
       "      <td>-0.052394</td>\n",
       "      <td>0.780884</td>\n",
       "      <td>2.728394</td>\n",
       "      <td>-0.802595</td>\n",
       "      <td>-3.781179</td>\n",
       "      <td>-1.861238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.599540</td>\n",
       "      <td>1.110806</td>\n",
       "      <td>-0.376205</td>\n",
       "      <td>-0.946819</td>\n",
       "      <td>-0.492917</td>\n",
       "      <td>0.446734</td>\n",
       "      <td>-0.399607</td>\n",
       "      <td>-0.879457</td>\n",
       "      <td>0.780884</td>\n",
       "      <td>2.534909</td>\n",
       "      <td>0.191274</td>\n",
       "      <td>0.138514</td>\n",
       "      <td>0.575870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.307150</td>\n",
       "      <td>1.809445</td>\n",
       "      <td>-0.654016</td>\n",
       "      <td>-1.722063</td>\n",
       "      <td>-0.492168</td>\n",
       "      <td>0.170437</td>\n",
       "      <td>-0.763368</td>\n",
       "      <td>-1.460798</td>\n",
       "      <td>-1.280599</td>\n",
       "      <td>-0.608647</td>\n",
       "      <td>-0.839616</td>\n",
       "      <td>0.138514</td>\n",
       "      <td>1.825358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.634642</td>\n",
       "      <td>0.491835</td>\n",
       "      <td>-0.131344</td>\n",
       "      <td>1.057468</td>\n",
       "      <td>2.723273</td>\n",
       "      <td>0.170437</td>\n",
       "      <td>-0.458384</td>\n",
       "      <td>-0.175645</td>\n",
       "      <td>-1.280599</td>\n",
       "      <td>-0.653035</td>\n",
       "      <td>-0.074294</td>\n",
       "      <td>0.138514</td>\n",
       "      <td>-0.754491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acousticness  danceability  duration_ms    energy  instrumentalness  \\\n",
       "0     -0.697633     -0.194548    -0.398940 -0.318116         -0.492359   \n",
       "1     -0.276291      0.295726    -0.374443 -0.795552          0.598355   \n",
       "2     -0.599540      1.110806    -0.376205 -0.946819         -0.492917   \n",
       "3     -0.307150      1.809445    -0.654016 -1.722063         -0.492168   \n",
       "4     -0.634642      0.491835    -0.131344  1.057468          2.723273   \n",
       "\n",
       "        key  liveness  loudness      mode  speechiness     tempo  \\\n",
       "0  1.275623 -0.737898  0.395794 -1.280599    -0.617752 -0.293827   \n",
       "1 -1.487342 -0.438792 -0.052394  0.780884     2.728394 -0.802595   \n",
       "2  0.446734 -0.399607 -0.879457  0.780884     2.534909  0.191274   \n",
       "3  0.170437 -0.763368 -1.460798 -1.280599    -0.608647 -0.839616   \n",
       "4  0.170437 -0.458384 -0.175645 -1.280599    -0.653035 -0.074294   \n",
       "\n",
       "   time_signature   valence  \n",
       "0        0.138514 -0.908149  \n",
       "1       -3.781179 -1.861238  \n",
       "2        0.138514  0.575870  \n",
       "3        0.138514  1.825358  \n",
       "4        0.138514 -0.754491  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()  # create feature trasformer object\n",
    "scaler.fit(X_train)  # fitting the transformer on the train split\n",
    "X_train_scaled = scaler.transform(X_train)  # transforming the train split\n",
    "X_test_scaled = scaler.transform(X_test)  # transforming the test split\n",
    "pd.DataFrame(X_train_scaled, columns=X_train.columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `fit` and `transform` paradigm for transformers\n",
    "- `sklearn` uses `fit` and `transform` paradigms for feature transformations. \n",
    "- We `fit` the transformer on the train split and then transform the train split as well as the test split. \n",
    "- We apply the same transformations on the test split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `sklearn` API summary: estimators\n",
    "\n",
    "Suppose `model` is a classification or regression model. \n",
    "\n",
    "```\n",
    "model.fit(X_train, y_train)\n",
    "X_train_predictions = model.predict(X_train)\n",
    "X_test_predictions = model.predict(X_test)\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `sklearn` API summary: transformers\n",
    "\n",
    "Suppose `transformer` is a transformer used to change the input representation, for example, to tackle missing values or to scales numeric features.\n",
    "\n",
    "```\n",
    "transformer.fit(X_train, [y_train])\n",
    "X_train_transformed = transformer.transform(X_train)\n",
    "X_test_transformed = transformer.transform(X_test)\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You can pass `y_train` in `fit` but it's usually ignored. It allows you to pass it just to be consistent with usual usage of `sklearn`'s `fit` method.   \n",
    "- You can also carry out fitting and transforming in one call using `fit_transform`. But be mindful to use it only on the train split and **not** on the test split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Do you expect `DummyClassifier` results to change after scaling the data? \n",
    "- Let's check whether scaling makes any difference for $k$-NNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.726\n",
      "Test score: 0.552\n"
     ]
    }
   ],
   "source": [
    "knn_unscaled = KNeighborsClassifier()\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "print(\"Train score: %0.3f\" % (knn_unscaled.score(X_train, y_train)))\n",
    "print(\"Test score: %0.3f\" % (knn_unscaled.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.798\n",
      "Test score: 0.686\n"
     ]
    }
   ],
   "source": [
    "knn_scaled = KNeighborsClassifier()\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "print(\"Train score: %0.3f\" % (knn_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test score: %0.3f\" % (knn_scaled.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The scores with scaled data are better compared to the unscaled data in case of $k$-NNs.\n",
    "- I am not carrying out cross-validation here for a reason that we'll look into soon. \n",
    "- Note that I am a bit sloppy here and using the test set several times for teaching purposes. But when you build an ML pipeline, please do assessment on the test set only once.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common preprocessing techniques\n",
    "\n",
    "Some commonly performed feature transformation include:  \n",
    "- Imputation: Tackling missing values\n",
    "- Scaling: Scaling of numeric features\n",
    "- One-hot encoding: Tackling categorical variables      \n",
    "    \n",
    "\n",
    "We can have one lecture on each of them! In this lesson our goal is to getting familiar with them so that we can use them to build ML pipelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the next part of this lecture, we'll build an ML pipeline using [California housing prices regression dataset](https://www.kaggle.com/harrywang/housing). In the process, we will talk about different feature transformations and how can we apply them so that we do not violate the golden rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset, splitting, and baseline\n",
    "\n",
    "We'll be working on [California housing prices regression dataset](https://www.kaggle.com/harrywang/housing) to demonstrate these feature transformation techniques. The task is to predict median house values in Californian districts, given a number of features from these districts. If you are running the notebook on your own, you'll have to download the data and put it in the data directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>-117.75</td>\n",
       "      <td>34.04</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2948.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>3.1250</td>\n",
       "      <td>113600.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20113</th>\n",
       "      <td>-119.57</td>\n",
       "      <td>37.94</td>\n",
       "      <td>17.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.4861</td>\n",
       "      <td>137500.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14289</th>\n",
       "      <td>-117.13</td>\n",
       "      <td>32.74</td>\n",
       "      <td>46.0</td>\n",
       "      <td>3355.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>2.6604</td>\n",
       "      <td>170100.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13665</th>\n",
       "      <td>-117.31</td>\n",
       "      <td>34.02</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>5.2139</td>\n",
       "      <td>129300.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14471</th>\n",
       "      <td>-117.23</td>\n",
       "      <td>32.88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5566.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>6303.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>1.8580</td>\n",
       "      <td>205000.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "6051     -117.75     34.04                22.0       2948.0           636.0   \n",
       "20113    -119.57     37.94                17.0        346.0           130.0   \n",
       "14289    -117.13     32.74                46.0       3355.0           768.0   \n",
       "13665    -117.31     34.02                18.0       1634.0           274.0   \n",
       "14471    -117.23     32.88                18.0       5566.0          1465.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "6051       2600.0       602.0         3.1250            113600.0   \n",
       "20113        51.0        20.0         3.4861            137500.0   \n",
       "14289      1457.0       708.0         2.6604            170100.0   \n",
       "13665       899.0       285.0         5.2139            129300.0   \n",
       "14471      6303.0      1458.0         1.8580            205000.0   \n",
       "\n",
       "      ocean_proximity  \n",
       "6051           INLAND  \n",
       "20113          INLAND  \n",
       "14289      NEAR OCEAN  \n",
       "13665          INLAND  \n",
       "14471      NEAR OCEAN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df = pd.read_csv(\"data/housing.csv\")\n",
    "train_df, test_df = train_test_split(housing_df, test_size=0.1, random_state=123)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Some column values are mean/median but some are not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's add some new features to the dataset which could help predicting the target: `median_house_value`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.assign(\n",
    "    rooms_per_household=train_df[\"total_rooms\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    rooms_per_household=test_df[\"total_rooms\"] / test_df[\"households\"]\n",
    ")\n",
    "\n",
    "train_df = train_df.assign(\n",
    "    bedrooms_per_household=train_df[\"total_bedrooms\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    bedrooms_per_household=test_df[\"total_bedrooms\"] / test_df[\"households\"]\n",
    ")\n",
    "\n",
    "train_df = train_df.assign(\n",
    "    population_per_household=train_df[\"population\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    population_per_household=test_df[\"population\"] / test_df[\"households\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>rooms_per_household</th>\n",
       "      <th>bedrooms_per_household</th>\n",
       "      <th>population_per_household</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>-117.75</td>\n",
       "      <td>34.04</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2948.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>3.1250</td>\n",
       "      <td>113600.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>4.897010</td>\n",
       "      <td>1.056478</td>\n",
       "      <td>4.318937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20113</th>\n",
       "      <td>-119.57</td>\n",
       "      <td>37.94</td>\n",
       "      <td>17.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.4861</td>\n",
       "      <td>137500.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>2.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14289</th>\n",
       "      <td>-117.13</td>\n",
       "      <td>32.74</td>\n",
       "      <td>46.0</td>\n",
       "      <td>3355.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>2.6604</td>\n",
       "      <td>170100.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "      <td>4.738701</td>\n",
       "      <td>1.084746</td>\n",
       "      <td>2.057910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13665</th>\n",
       "      <td>-117.31</td>\n",
       "      <td>34.02</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>5.2139</td>\n",
       "      <td>129300.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>5.733333</td>\n",
       "      <td>0.961404</td>\n",
       "      <td>3.154386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14471</th>\n",
       "      <td>-117.23</td>\n",
       "      <td>32.88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5566.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>6303.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>1.8580</td>\n",
       "      <td>205000.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "      <td>3.817558</td>\n",
       "      <td>1.004801</td>\n",
       "      <td>4.323045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "6051     -117.75     34.04                22.0       2948.0           636.0   \n",
       "20113    -119.57     37.94                17.0        346.0           130.0   \n",
       "14289    -117.13     32.74                46.0       3355.0           768.0   \n",
       "13665    -117.31     34.02                18.0       1634.0           274.0   \n",
       "14471    -117.23     32.88                18.0       5566.0          1465.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "6051       2600.0       602.0         3.1250            113600.0   \n",
       "20113        51.0        20.0         3.4861            137500.0   \n",
       "14289      1457.0       708.0         2.6604            170100.0   \n",
       "13665       899.0       285.0         5.2139            129300.0   \n",
       "14471      6303.0      1458.0         1.8580            205000.0   \n",
       "\n",
       "      ocean_proximity  rooms_per_household  bedrooms_per_household  \\\n",
       "6051           INLAND             4.897010                1.056478   \n",
       "20113          INLAND            17.300000                6.500000   \n",
       "14289      NEAR OCEAN             4.738701                1.084746   \n",
       "13665          INLAND             5.733333                0.961404   \n",
       "14471      NEAR OCEAN             3.817558                1.004801   \n",
       "\n",
       "       population_per_household  \n",
       "6051                   4.318937  \n",
       "20113                  2.550000  \n",
       "14289                  2.057910  \n",
       "13665                  3.154386  \n",
       "14471                  4.323045  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When is it OK to do things before splitting? \n",
    "\n",
    "- Here it would have been OK to add new features before splitting because we are not using any global information in the data but only looking at one row at a time. \n",
    "- But just to be safe and to avoid accidentally breaking the golden rule, it's better to do it after splitting. \n",
    "\n",
    "- Question: Should we remove `total_rooms`, `total_bedrooms`, and `population` columns? \n",
    "    - Probably. But I am keeping them in this lecture. You could experiment with removing them and examine whether results change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "      <th>rooms_per_household</th>\n",
       "      <th>bedrooms_per_household</th>\n",
       "      <th>population_per_household</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>-117.75</td>\n",
       "      <td>34.04</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2948.0</td>\n",
       "      <td>636.0</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>3.1250</td>\n",
       "      <td>113600.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>4.897010</td>\n",
       "      <td>1.056478</td>\n",
       "      <td>4.318937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20113</th>\n",
       "      <td>-119.57</td>\n",
       "      <td>37.94</td>\n",
       "      <td>17.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.4861</td>\n",
       "      <td>137500.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>2.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14289</th>\n",
       "      <td>-117.13</td>\n",
       "      <td>32.74</td>\n",
       "      <td>46.0</td>\n",
       "      <td>3355.0</td>\n",
       "      <td>768.0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>2.6604</td>\n",
       "      <td>170100.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "      <td>4.738701</td>\n",
       "      <td>1.084746</td>\n",
       "      <td>2.057910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13665</th>\n",
       "      <td>-117.31</td>\n",
       "      <td>34.02</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>5.2139</td>\n",
       "      <td>129300.0</td>\n",
       "      <td>INLAND</td>\n",
       "      <td>5.733333</td>\n",
       "      <td>0.961404</td>\n",
       "      <td>3.154386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14471</th>\n",
       "      <td>-117.23</td>\n",
       "      <td>32.88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5566.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>6303.0</td>\n",
       "      <td>1458.0</td>\n",
       "      <td>1.8580</td>\n",
       "      <td>205000.0</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "      <td>3.817558</td>\n",
       "      <td>1.004801</td>\n",
       "      <td>4.323045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "6051     -117.75     34.04                22.0       2948.0           636.0   \n",
       "20113    -119.57     37.94                17.0        346.0           130.0   \n",
       "14289    -117.13     32.74                46.0       3355.0           768.0   \n",
       "13665    -117.31     34.02                18.0       1634.0           274.0   \n",
       "14471    -117.23     32.88                18.0       5566.0          1465.0   \n",
       "\n",
       "       population  households  median_income  median_house_value  \\\n",
       "6051       2600.0       602.0         3.1250            113600.0   \n",
       "20113        51.0        20.0         3.4861            137500.0   \n",
       "14289      1457.0       708.0         2.6604            170100.0   \n",
       "13665       899.0       285.0         5.2139            129300.0   \n",
       "14471      6303.0      1458.0         1.8580            205000.0   \n",
       "\n",
       "      ocean_proximity  rooms_per_household  bedrooms_per_household  \\\n",
       "6051           INLAND             4.897010                1.056478   \n",
       "20113          INLAND            17.300000                6.500000   \n",
       "14289      NEAR OCEAN             4.738701                1.084746   \n",
       "13665          INLAND             5.733333                0.961404   \n",
       "14471      NEAR OCEAN             3.817558                1.004801   \n",
       "\n",
       "       population_per_household  \n",
       "6051                   4.318937  \n",
       "20113                  2.550000  \n",
       "14289                  2.057910  \n",
       "13665                  3.154386  \n",
       "14471                  4.323045  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature scales are quite different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18576 entries, 6051 to 19966\n",
      "Data columns (total 13 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   longitude                 18576 non-null  float64\n",
      " 1   latitude                  18576 non-null  float64\n",
      " 2   housing_median_age        18576 non-null  float64\n",
      " 3   total_rooms               18576 non-null  float64\n",
      " 4   total_bedrooms            18391 non-null  float64\n",
      " 5   population                18576 non-null  float64\n",
      " 6   households                18576 non-null  float64\n",
      " 7   median_income             18576 non-null  float64\n",
      " 8   median_house_value        18576 non-null  float64\n",
      " 9   ocean_proximity           18576 non-null  object \n",
      " 10  rooms_per_household       18576 non-null  float64\n",
      " 11  bedrooms_per_household    18391 non-null  float64\n",
      " 12  population_per_household  18576 non-null  float64\n",
      "dtypes: float64(12), object(1)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one categorical feature and all other features are numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>rooms_per_household</th>\n",
       "      <th>bedrooms_per_household</th>\n",
       "      <th>population_per_household</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18576.000000</td>\n",
       "      <td>18576.000000</td>\n",
       "      <td>18576.000000</td>\n",
       "      <td>18576.000000</td>\n",
       "      <td>18391.000000</td>\n",
       "      <td>18576.000000</td>\n",
       "      <td>18576.000000</td>\n",
       "      <td>18576.000000</td>\n",
       "      <td>18576.000000</td>\n",
       "      <td>18576.000000</td>\n",
       "      <td>18391.000000</td>\n",
       "      <td>18576.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-119.565888</td>\n",
       "      <td>35.627966</td>\n",
       "      <td>28.622255</td>\n",
       "      <td>2635.749677</td>\n",
       "      <td>538.229786</td>\n",
       "      <td>1428.578165</td>\n",
       "      <td>500.061100</td>\n",
       "      <td>3.862552</td>\n",
       "      <td>206292.067991</td>\n",
       "      <td>5.426067</td>\n",
       "      <td>1.097516</td>\n",
       "      <td>3.052349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.999622</td>\n",
       "      <td>2.134658</td>\n",
       "      <td>12.588307</td>\n",
       "      <td>2181.789934</td>\n",
       "      <td>421.805266</td>\n",
       "      <td>1141.664801</td>\n",
       "      <td>383.044313</td>\n",
       "      <td>1.892491</td>\n",
       "      <td>115083.856175</td>\n",
       "      <td>2.512319</td>\n",
       "      <td>0.486266</td>\n",
       "      <td>10.020873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-124.350000</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>14999.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-121.790000</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1449.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>788.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>2.560225</td>\n",
       "      <td>119400.000000</td>\n",
       "      <td>4.439360</td>\n",
       "      <td>1.005888</td>\n",
       "      <td>2.430323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-118.490000</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>2127.000000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>3.527500</td>\n",
       "      <td>179300.000000</td>\n",
       "      <td>5.226415</td>\n",
       "      <td>1.048860</td>\n",
       "      <td>2.818868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-118.010000</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3145.000000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>1727.000000</td>\n",
       "      <td>606.000000</td>\n",
       "      <td>4.736900</td>\n",
       "      <td>263600.000000</td>\n",
       "      <td>6.051620</td>\n",
       "      <td>1.099723</td>\n",
       "      <td>3.283921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-114.310000</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>39320.000000</td>\n",
       "      <td>6445.000000</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>6082.000000</td>\n",
       "      <td>15.000100</td>\n",
       "      <td>500001.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>1243.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          longitude      latitude  housing_median_age   total_rooms  \\\n",
       "count  18576.000000  18576.000000        18576.000000  18576.000000   \n",
       "mean    -119.565888     35.627966           28.622255   2635.749677   \n",
       "std        1.999622      2.134658           12.588307   2181.789934   \n",
       "min     -124.350000     32.540000            1.000000      2.000000   \n",
       "25%     -121.790000     33.930000           18.000000   1449.000000   \n",
       "50%     -118.490000     34.250000           29.000000   2127.000000   \n",
       "75%     -118.010000     37.710000           37.000000   3145.000000   \n",
       "max     -114.310000     41.950000           52.000000  39320.000000   \n",
       "\n",
       "       total_bedrooms    population    households  median_income  \\\n",
       "count    18391.000000  18576.000000  18576.000000   18576.000000   \n",
       "mean       538.229786   1428.578165    500.061100       3.862552   \n",
       "std        421.805266   1141.664801    383.044313       1.892491   \n",
       "min          1.000000      3.000000      1.000000       0.499900   \n",
       "25%        296.000000    788.000000    280.000000       2.560225   \n",
       "50%        435.000000   1167.000000    410.000000       3.527500   \n",
       "75%        647.000000   1727.000000    606.000000       4.736900   \n",
       "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
       "\n",
       "       median_house_value  rooms_per_household  bedrooms_per_household  \\\n",
       "count        18576.000000         18576.000000            18391.000000   \n",
       "mean        206292.067991             5.426067                1.097516   \n",
       "std         115083.856175             2.512319                0.486266   \n",
       "min          14999.000000             0.846154                0.333333   \n",
       "25%         119400.000000             4.439360                1.005888   \n",
       "50%         179300.000000             5.226415                1.048860   \n",
       "75%         263600.000000             6.051620                1.099723   \n",
       "max         500001.000000           141.909091               34.066667   \n",
       "\n",
       "       population_per_household  \n",
       "count              18576.000000  \n",
       "mean                   3.052349  \n",
       "std                   10.020873  \n",
       "min                    0.692308  \n",
       "25%                    2.430323  \n",
       "50%                    2.818868  \n",
       "75%                    3.283921  \n",
       "max                 1243.333333  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Seems like total_bedrooms column has some missing values. \n",
    "- This must have affected our new feature `bedrooms_per_household` as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df[\"total_bedrooms\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## (optional)\n",
    "train_df.hist(bins=50, figsize=(20, 15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## (optional)\n",
    "train_df.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    alpha=0.4,\n",
    "    s=train_df[\"population\"] / 100,\n",
    "    figsize=(10, 7),\n",
    "    c=\"median_house_value\",\n",
    "    cmap=plt.get_cmap(\"jet\"),\n",
    "    colorbar=True,\n",
    "    sharex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What all transformations we need to apply on the dataset? \n",
    "\n",
    "Here is what we see from the EDA. \n",
    "\n",
    "- Some missing values in `total_bedrooms` column\n",
    "- Scales are quite different across columns. \n",
    "- Categorical variable `ocean_proximity`\n",
    "\n",
    "Read about [preprocessing techniques implemented in `scikit-learn`](https://scikit-learn.org/stable/modules/preprocessing.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are droping the categorical variable ocean_proximity for now. We'll come back to it in a bit.\n",
    "X_train = train_df.drop(columns=[\"median_house_value\", \"ocean_proximity\"])\n",
    "y_train = train_df[\"median_house_value\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"median_house_value\", \"ocean_proximity\"])\n",
    "y_test = test_df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's first run our baseline model `DummyRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}  # dictionary to store our results for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "    \"\"\"\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to store mean cross-validation validation values\n",
    "def store_cross_val_results(model_name, scores, results_dict):\n",
    "    \"\"\"\n",
    "    Stores mean scores from cross_validate in results_dict for\n",
    "    the given model model_name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name :\n",
    "        scikit-learn classification model\n",
    "    scores : dict\n",
    "        object return by `cross_validate`\n",
    "    results_dict: dict\n",
    "        dictionary to store results\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    results_dict[model_name] = {\n",
    "        \"mean_train_score\": \"{:0.4f}\".format(np.mean(scores[\"train_score\"])),\n",
    "        \"mean_validation_score\": \"{:0.4f}\".format(np.mean(scores[\"test_score\"])),\n",
    "        \"mean_fit_time (s)\": \"{:0.4f}\".format(np.mean(scores[\"fit_time\"])),\n",
    "        \"mean_score_time (s)\": \"{:0.4f}\".format(np.mean(scores[\"score_time\"])),\n",
    "        \"std_train_score\": \"{:0.4f}\".format(scores[\"train_score\"].std()),\n",
    "        \"std_validation_score\": \"{:0.4f}\".format(scores[\"test_score\"].std()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dummy = DummyRegressor(strategy=\"median\")\n",
    "results[\"Dummy\"] = mean_std_cross_val_scores(\n",
    "    dummy, X_train, y_train, return_train_score=True\n",
    ")\n",
    "#scores = cross_validate(dummy, X_train, y_train, return_train_score=True)\n",
    "#store_cross_val_results(\"dummy\", scores, results_dict)\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "# knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's the problem? \n",
    "\n",
    "```\n",
    "ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
    "```\n",
    "\n",
    "- The classifier is not able to deal with missing values (NaNs).\n",
    "- What are possible ways to deal with the problem? \n",
    "    - Delete the rows? \n",
    "    - Replace them with some reasonable values?    \n",
    "- `SimpleImputer` is a transformer in `sklearn` to deal with this problem. For example, \n",
    "    - You can impute missing values in categorical columns with the most frequent value.\n",
    "    - You can impute the missing values in numeric columns with the mean or median of the column.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train.sort_values(\"total_bedrooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(X_train)\n",
    "X_train_imp = imputer.transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's check whether the NaN values have been replaced or not\n",
    "- Note that `imputer.transform` returns an `numpy` array and not a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train_imp, columns=X_train.columns, index=X_train.index)\n",
    "df.loc[7763]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train KNN on the imputed values\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_imp, y_train)\n",
    "knn.score(X_train_imp, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- This problem affects a large number of ML methods.\n",
    "- A number of approaches to this problem. We are going to look into two most popular ones.  \n",
    "\n",
    "| Approach | What it does | How to update $X$ (but see below!) | sklearn implementation | \n",
    "|---------|------------|-----------------------|----------------|\n",
    "| normalization | sets range to $[0,1]$   | `X -= np.min(X,axis=0)`<br>`X /= np.max(X,axis=0)`  | [`MinMaxScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "| standardization | sets sample mean to $0$, s.d. to $1$   | `X -= np.mean(X,axis=0)`<br>`X /=  np.std(X,axis=0)` | [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) |\n",
    "\n",
    "There are all sorts of articles on this; see, e.g. [here](http://www.dataminingblog.com/standardization-vs-normalization/) and [here](https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src='./img/scaling-data.png' width=\"600\">\n",
    "\n",
    "[source](https://amueller.github.io/COMS4995-s19/slides/aml-05-preprocessing/#8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imp)\n",
    "X_test_scaled = scaler.transform(X_test_imp)\n",
    "pd.DataFrame(X_train_scaled, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "knn.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imp)\n",
    "X_test_scaled = scaler.transform(X_test_imp)\n",
    "pd.DataFrame(X_train_scaled, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "knn.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Big difference in the KNN training performance after scaling the data. \n",
    "- But we saw last week that training score doesn't tell us much. We should look at the cross-validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Question\n",
    "\n",
    "- What would happen if you apply `StandardScaler` on sparse data? \n",
    "    - Sparse data is the data where there are only a few non-zero values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature transformations and the golden rule "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to carry out cross-validation? \n",
    "\n",
    "- Last week we saw that cross validation is a better way to get a realistic assessment of the model. \n",
    "- Let's try cross-validation with transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "scores = cross_validate(knn, X_train_scaled, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do you see any problem here? \n",
    "- Are we applying `fit_transform` on train portion and `transform` on validation portion in each fold?  \n",
    "    - Here you might be allowing information from the validation set to **leak** into the training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- You need to apply the **SAME** preprocessing steps to train/validation.\n",
    "- With many different transformations and cross validation the code gets unwieldy very quickly. \n",
    "- Likely to make mistakes and \"leak\" information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bad methodology 1: Scaling the data separately (for class discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imp)\n",
    "X_train_scaled = scaler.transform(X_train_imp)\n",
    "\n",
    "scaler = StandardScaler()  # Creating a separate object for scaling test data. Bad!\n",
    "scaler.fit(X_test_imp)  # Calling fit on the test data. NOOOO!\n",
    "X_test_scaled = scaler.transform(\n",
    "    X_test_imp\n",
    ")  # Transforming the test data using the scaler fit on test data ... Bad!\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print(f\"Training score: {knn.score(X_train_scaled, y_train):.2f}\")\n",
    "print(f\"Test score: {knn.score(X_test_scaled, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Question\n",
    "- Is anything wrong in methodology 1? If yes, what is it?  \n",
    "\n",
    "**Varada's answer:**\n",
    "\n",
    "Yes. We are scaling the train and test splits separately using two different `StandardScaler` objects. This is bad because we want to apply the same transformation on the training and test splits.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bad methodology 2: Scaling the data together (for class discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train_imp.shape, X_test_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# join the train and test sets back together\n",
    "XX = np.vstack((X_train_imp, X_test_imp))  ## Don't do it!\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(XX)\n",
    "XX_scaled = scaler.transform(XX)\n",
    "XX_train = XX_scaled[:18576]\n",
    "XX_test = XX_scaled[18576:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "knn.fit(XX_train, y_train)\n",
    "\n",
    "print(f\"Training score: {knn.score(XX_train, y_train):.2f}\")  # Misleading score\n",
    "print(f\"Test score: {knn.score(XX_test, y_test):.2f}\")  # Misleading score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Question\n",
    "- Is anything wrong in methodology 2? If yes, what is it? \n",
    "\n",
    "**Varada's answer:**\n",
    "Yes. Here we are scaling the train and test splits together. The golden rule says that the test data shouldn't influence the training in any way. Here we are using the information from the test split when we `fit` the scaler and calculate the mean, as we are passing the combined `X_train` and `X_test` to it. So it's violation of the golden rule.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- In these examples our test accuracies look fine, but our methodology is flawed.\n",
    "- Implications can be significant in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Methodology 3 (for class discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "\n",
    "imp1 = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "imp1.fit(X_train)\n",
    "X_train_imp1 = imp1.transform(X_train)\n",
    "X_test_imp1 = imp1.transform(X_test)\n",
    "scores = cross_validate(knn, X_train_imp1, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Is anything wrong in methodology 3? Are we breaking the golden rule here? \n",
    "\n",
    "**Varada's answer:**\n",
    "\n",
    "This is tricky. If we were using a different strategy, for instance, `strategy=\"mean\"` we would have violated the golden rule. But since \"constant\" strategy doesn't really make any use of the rows from the validation/test splits and works on a row-by-row basis, we are not breaking the golden rule here. That said, in general be very careful when you pass preprocessed data to `cross_validate`, as you are likely to be breaking the golden rule if you do so. Imagine that your imputation strategy were \"mean\". In that case,  \n",
    "- `imp1.fit(X_train)` will calculate means of all columns in X_train using all the rows in `X_train`\n",
    "- `imp1.transform(X_train)` will transform the data to create `X_train_imp1`. \n",
    "- When you call `cross_validate(knn, X_train_imp1, y_train, return_train_score=True)`, inside `cross_validate`, train and validation splits will be created for each fold from `X_train_imp1`. \n",
    "- Remember that we shouldn't be including the validation split when we call `fit` both for transformers and for estimators. \n",
    "- Are we doing it here? No. You already have used rows from the validation split in each fold when you `fit` the imputer with `mean` strategy outside `cross_validate`. And the rows from validation split have been used to calculate the mean. \n",
    "- So your training inside cross-validation in each fold would be influenced by the information in the validation split in that fold, which is violation of the golden rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pipelines\n",
    "\n",
    "Can we do this in a more elegant and organized way?\n",
    "\n",
    "- YES!! Using [`scikit-learn Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "- [`scikit-learn Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) allows you to define a \"pipeline\" of transformers with a final estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's combine the preprocessing and model with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "### Simple example of a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"reg\", KNeighborsRegressor()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Syntax: pass in a list of steps.\n",
    "- The last step should be a **model/classifier/regressor**.\n",
    "- All the earlier steps should be **transformers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we are passing `X_train` and **not** the imputed or scaled data here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "When you call `fit` the pipeline is carrying out the following steps:\n",
    "\n",
    "- Fit `SimpleImputer` on `X_train`\n",
    "- Transform `X_train` using the fit `SimpleImputer` to create `X_train_imp`\n",
    "- Fit `StandardScaler` on `X_train_imp`\n",
    "- Transform `X_train_imp` using the fit `StandardScaler` to create `X_train_imp_scaled`\n",
    "- Fit the model (`KNeighborsRegressor` in our case) on `X_train_imp_scaled`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are passing original data to `predict` as well. This time the pipeline is carrying out following steps:\n",
    "- Transform `X_train` using the fit `SimpleImputer` to create `X_train_imp`\n",
    "- Transform `X_train_imp` using the fit `StandardScaler` to create `X_train_imp_scaled`\n",
    "- Predict using the fit model (`KNeighborsRegressor` in our case) on `X_train_imp_scaled`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src='./img/pipeline.png' width=\"800\">\n",
    "\n",
    "[Source](https://amueller.github.io/COMS4995-s20/slides/aml-04-preprocessing/#18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's try cross validation with pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "store_cross_val_results(\"imp + scaling + knn\", scores, results_dict)\n",
    "pd.DataFrame(results_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using a `Pipeline` takes care of applying the `fit_transform` on the train portion and only `transform` on the validation portion in each fold.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Categorical variables <a name=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that we had dropped the categorical feature `ocean_proximity` feature from the dataframe. But it could potentially be a useful feature in this task. \n",
    "\n",
    "- Let's create our `X_train` and and `X_test` again by keeping the feature in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"median_house_value\"])\n",
    "y_train = train_df[\"median_house_value\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"median_house_value\"])\n",
    "y_test = test_df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's try to build a `KNeighborRegressor` on this data using our pipeline\n",
    "- This will fail because we have non-numeric data.\n",
    "- Imagine how $k$-NN would calculate distances when you have non-numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe.fit(X_train, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Can we use this feature in the model? \n",
    "- In `scikit-learn`, most algorithms require numeric inputs.\n",
    "- Decision trees could theoretically work with categorical features.  \n",
    "    - However, the sklearn implementation does not support this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### What are the options? \n",
    "\n",
    "- Drop the column (not recommended)\n",
    "    - If you know that the column is not relevant to the target in any way you may drop it. \n",
    "- We can transform categorical features to numeric ones so that we can use them in the model.     \n",
    "    - [Ordinal encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) (occasionally recommended)\n",
    "    - One-hot encoding (recommended in most cases) (this lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_toy = pd.DataFrame(\n",
    "    {\n",
    "        \"language\": [\n",
    "            \"English\",\n",
    "            \"Vietnamese\",\n",
    "            \"English\",\n",
    "            \"Mandarin\",\n",
    "            \"English\",\n",
    "            \"English\",\n",
    "            \"Mandarin\",\n",
    "            \"English\",\n",
    "            \"Vietnamese\",\n",
    "            \"Mandarin\",\n",
    "            \"French\",\n",
    "            \"Spanish\",\n",
    "            \"Mandarin\",\n",
    "            \"Hindi\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "X_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ordinal encoding (occasionally recommended)\n",
    "\n",
    "- Here we simply assign an integer to each of our unique categorical labels. \n",
    "- We can use sklearn's [`OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(X_toy)\n",
    "X_toy_ord = enc.transform(X_toy)\n",
    "df = pd.DataFrame(\n",
    "    data=X_toy_ord,\n",
    "    columns=[\"language_enc\"],\n",
    "    index=X_toy.index,\n",
    ")\n",
    "pd.concat([X_toy, df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the problem with this approach? \n",
    "- We have imposed ordinality on the categorical data.\n",
    "- For example, imagine when you are calculating distances. Is it fair to say that French and Hindi are closer than French and Spanish? \n",
    "- In general, label encoding is useful if there is ordinality in your data and capturing it is important for your problem, e.g., `[cold, warm, hot]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### One-hot encoding (OHE)\n",
    "- Create new binary columns to represent our categories.\n",
    "- If we have $c$ categories in our column.\n",
    "    - We create $c$ new binary columns to represent those categories.\n",
    "- Example: Imagine a language column which has the information on whether you \n",
    "\n",
    "- We can use sklearn's [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "enc.fit(X_toy)\n",
    "X_toy_ohe = enc.transform(X_toy)\n",
    "pd.DataFrame(\n",
    "    data=X_toy_ohe,\n",
    "    columns=enc.get_feature_names([\"language\"]),\n",
    "    index=X_toy.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's do it on our housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, dtype=\"int\")\n",
    "ohe.fit(X_train[[\"ocean_proximity\"]])\n",
    "X_imp_ohe_train = ohe.transform(X_train[[\"ocean_proximity\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can look at the new features created using `categories_` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_ohe = pd.DataFrame(\n",
    "    data=X_imp_ohe_train,\n",
    "    columns=ohe.get_feature_names([\"ocean_proximity\"]),\n",
    "    index=X_train.index,\n",
    ")\n",
    "transformed_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do we put this together with other columns in the data before fitting regressor? \n",
    "- We want to apply different transformations to different columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: Different transformations on different columns\n",
    "\n",
    "- Before we fit our regressor, we want to apply different transformations on different columns \n",
    "    - Numeric columns\n",
    "        - imputation \n",
    "        - scaling         \n",
    "    - Categorical columns \n",
    "        - imputation \n",
    "        - one-hot encoding \n",
    "        \n",
    "**Coming up: sklearn's [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)!!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `ColumnTransformer` <a name=\"7\"></a>\n",
    "- sklearn's [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) makes this more manageable.\n",
    "    - A big advantage here is that we build all our transformations together into one object, and that way we're sure we do the same operations to all splits of the data. \n",
    "    - Otherwise we might, for example, do the OHE on both train and test but forget to scale the test data.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src='./img/column-transformer.png' width=\"1500\">\n",
    "\n",
    "[Adapted from here.](https://amueller.github.io/COMS4995-s20/slides/aml-04-preprocessing/#37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the categorical and numeric columns\n",
    "numeric_features = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "    \"rooms_per_household\",\n",
    "    \"bedrooms_per_household\",\n",
    "    \"population_per_household\",\n",
    "]\n",
    "\n",
    "categorical_features = [\"ocean_proximity\"]\n",
    "# reamainder_features = [\"median_income\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's build a pipeline for our dataset\n",
    "- create the preprocessing pipelines for both numeric and categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    # remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `ColumnTransformer` syntax is somewhat similar to Pipeline in that you pass in a list of tuples.\n",
    "- But here each tuple has 3 values instead of 2: (name, object, list of columns)\n",
    "\n",
    "- A big advantage here is that we build all our transformations together into one object, and that way we're sure we do the same operations to all splits of the data.\n",
    "\n",
    "- Otherwise we might, for example, do the OHE on both train and test but forget to scale the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we `fit` with the preprocessor, it calls `fit` on _all_ the transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pp = preprocessor.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we transform with the preprocessor, it calls `transform` on _all_ the transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can get the new names of the columns that were generated by the one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names(\n",
    "    categorical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this with the numeric feature names gives us all the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = numeric_features + list(\n",
    "    preprocessor.named_transformers_[\"cat\"]\n",
    "    .named_steps[\"onehot\"]\n",
    "    .get_feature_names(categorical_features)\n",
    ")\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        # (\"reg\", KNeighborsRegressor()),\n",
    "        (\"reg\", SVR(gamma=0.01)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "store_cross_val_results(\"imp + scaling + ohe + SVR\", scores, results_dict)\n",
    "pd.DataFrame(results_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that categorical features are different than free text features. Sometimes there are columns containing free text information and we we'll look at ways to deal with them later in the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `remainder=\"passthrough\"`\n",
    "- Side note: the `ColumnTransformer` will automatically remove columns that are not being transformed:\n",
    "- Use `remainder=\"passthrough\"` of `ColumnTransformer` to keep the other columns in tact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Preprocessing the targets?\n",
    "\n",
    "- Generally no need for this when doing classification. \n",
    "- In regression it makes sense in some cases. More on this in 573. \n",
    "- `sklearn` is fine with categorical labels ($y$-values) for classification problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Revisit: Lecture learning objectives\n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- identify when to implement feature transformations such as imputation, scaling, and one-hot encoding in a machine learning model development pipeline; \n",
    "- use `sklearn` for applying feature transformations on your dataset;\n",
    "- discuss golden rule in the context of feature transformations;\n",
    "- use `sklearn.pipeline.Pipeline` to build a preliminary machine learning pipeline;   \n",
    "- use `ColumnTransformer` to build all our transformations together into one object and use it with `sklearn` pipelines.  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
