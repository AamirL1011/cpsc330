{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Lecture 6: `sklearn` `ColumnTransformer` and Text Features\n",
    "\n",
    "UBC 2020-21\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    Normalizer,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    "    normalize,\n",
    "    scale,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Learning objectives \n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- use `ColumnTransformer` to build all our transformations together into one object and use it with `sklearn` pipelines.  \n",
    "1. explain `handle_unknown=\"ignore\"` hyperparameter of `scikit-learn`'s `OneHotEncoder`;\n",
    "2. identify when it's appropriate to apply ordinal encoding vs one-hot encoding;\n",
    "3. explain strategies to deal with categorical variables with too many categories; \n",
    "4. explain why text data needs a different treatment than categorical variables;\n",
    "5. use `scikit-learn`'s `CountVectorizer` to encode text data;\n",
    "5. explain different hyperparameters of `CountVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## sklearn's [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In most applications, some features are categorical, some are continuous, some are binary, and some are ordinal. \n",
    "\n",
    "- When we want to develop supervised machine learning pipelines on real-world datasets, very often we want to apply different transformation on different columns. \n",
    "\n",
    "- Enter `sklearn`'s `ColumnTransformer`!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Let's look at a toy example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ml_experience</th>\n",
       "      <th>major</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>university_years</th>\n",
       "      <th>lab1</th>\n",
       "      <th>lab2</th>\n",
       "      <th>lab3</th>\n",
       "      <th>lab4</th>\n",
       "      <th>quiz1</th>\n",
       "      <th>quiz2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>93.0</td>\n",
       "      <td>84</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Average</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80</td>\n",
       "      <td>83</td>\n",
       "      <td>91</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Poor</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "      <td>89</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Economics</td>\n",
       "      <td>Good</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>73.0</td>\n",
       "      <td>68</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>88.0</td>\n",
       "      <td>89</td>\n",
       "      <td>88</td>\n",
       "      <td>91</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Poor</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>93.0</td>\n",
       "      <td>69</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Linguistics</td>\n",
       "      <td>Average</td>\n",
       "      <td>2</td>\n",
       "      <td>97</td>\n",
       "      <td>90.0</td>\n",
       "      <td>94</td>\n",
       "      <td>82</td>\n",
       "      <td>80</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Average</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>82.0</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>85</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>Psycology</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>86.0</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>78</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Physics</td>\n",
       "      <td>Average</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>88.0</td>\n",
       "      <td>93</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Physics</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>2</td>\n",
       "      <td>98</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>94.0</td>\n",
       "      <td>96</td>\n",
       "      <td>95</td>\n",
       "      <td>100</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Poor</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>90.0</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Good</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>85.0</td>\n",
       "      <td>67</td>\n",
       "      <td>94</td>\n",
       "      <td>92</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Average</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>91.0</td>\n",
       "      <td>93</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>Economics</td>\n",
       "      <td>Average</td>\n",
       "      <td>3</td>\n",
       "      <td>86</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65</td>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>Psycology</td>\n",
       "      <td>Good</td>\n",
       "      <td>2</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>Psycology</td>\n",
       "      <td>Poor</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>94.0</td>\n",
       "      <td>87</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>Linguistics</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92</td>\n",
       "      <td>96</td>\n",
       "      <td>87</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ml_experience                   major class_attendance  university_years  \\\n",
       "0               1        Computer Science        Excellent                 3   \n",
       "1               1  Mechanical Engineering          Average                 2   \n",
       "2               0             Mathematics             Poor                 3   \n",
       "3               0             Mathematics        Excellent                 3   \n",
       "4               0              Psychology             Good                 4   \n",
       "5               1               Economics             Good                 5   \n",
       "6               1        Computer Science        Excellent                 4   \n",
       "7               0  Mechanical Engineering             Poor                 3   \n",
       "8               0             Linguistics          Average                 2   \n",
       "9               1             Mathematics          Average                 4   \n",
       "10              0               Psycology             Good                 3   \n",
       "11              1                 Physics          Average                 1   \n",
       "12              1                 Physics        Excellent                 2   \n",
       "13              0  Mechanical Engineering        Excellent                 4   \n",
       "14              0             Mathematics             Poor                 3   \n",
       "15              1        Computer Science             Good                 3   \n",
       "16              0        Computer Science          Average                 5   \n",
       "17              1               Economics          Average                 3   \n",
       "18              1               Psycology             Good                 2   \n",
       "19              0               Psycology             Poor                 2   \n",
       "20              1             Linguistics        Excellent                 4   \n",
       "\n",
       "    lab1  lab2  lab3  lab4  quiz1   quiz2  \n",
       "0     92  93.0    84    91     92      A+  \n",
       "1     94  90.0    80    83     91  not A+  \n",
       "2     78  85.0    83    80     80  not A+  \n",
       "3     91   NaN    92    91     89      A+  \n",
       "4     77  83.0    90    92     85      A+  \n",
       "5     70  73.0    68    74     71  not A+  \n",
       "6     80  88.0    89    88     91      A+  \n",
       "7     95  93.0    69    79     75  not A+  \n",
       "8     97  90.0    94    82     80  not A+  \n",
       "9     95  82.0    94    94     85  not A+  \n",
       "10    98  86.0    95    95     78      A+  \n",
       "11    95  88.0    93    92     85      A+  \n",
       "12    98  96.0    96    99    100      A+  \n",
       "13    95  94.0    96    95    100      A+  \n",
       "14    95  90.0    93    95     70  not A+  \n",
       "15    92  85.0    67    94     92  not A+  \n",
       "16    75  91.0    93    86     85      A+  \n",
       "17    86  89.0    65    86     87  not A+  \n",
       "18    91   NaN    90    88     82  not A+  \n",
       "19    77  94.0    87    81     89  not A+  \n",
       "20    96  92.0    92    96     87      A+  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/quiz2-grade-toy-col-transformer.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21 entries, 0 to 20\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   ml_experience     21 non-null     int64  \n",
      " 1   major             21 non-null     object \n",
      " 2   class_attendance  21 non-null     object \n",
      " 3   university_years  21 non-null     int64  \n",
      " 4   lab1              21 non-null     int64  \n",
      " 5   lab2              19 non-null     float64\n",
      " 6   lab3              21 non-null     int64  \n",
      " 7   lab4              21 non-null     int64  \n",
      " 8   quiz1             21 non-null     int64  \n",
      " 9   quiz2             21 non-null     object \n",
      "dtypes: float64(1), int64(6), object(3)\n",
      "memory usage: 1.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transformations on the toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ml_experience</th>\n",
       "      <th>major</th>\n",
       "      <th>class_attendance</th>\n",
       "      <th>university_years</th>\n",
       "      <th>lab1</th>\n",
       "      <th>lab2</th>\n",
       "      <th>lab3</th>\n",
       "      <th>lab4</th>\n",
       "      <th>quiz1</th>\n",
       "      <th>quiz2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>93.0</td>\n",
       "      <td>84</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>Average</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>90.0</td>\n",
       "      <td>80</td>\n",
       "      <td>83</td>\n",
       "      <td>91</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Poor</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>not A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Mathematics</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "      <td>89</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>83.0</td>\n",
       "      <td>90</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>A+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ml_experience                   major class_attendance  university_years  \\\n",
       "0              1        Computer Science        Excellent                 3   \n",
       "1              1  Mechanical Engineering          Average                 2   \n",
       "2              0             Mathematics             Poor                 3   \n",
       "3              0             Mathematics        Excellent                 3   \n",
       "4              0              Psychology             Good                 4   \n",
       "\n",
       "   lab1  lab2  lab3  lab4  quiz1   quiz2  \n",
       "0    92  93.0    84    91     92      A+  \n",
       "1    94  90.0    80    83     91  not A+  \n",
       "2    78  85.0    83    80     80  not A+  \n",
       "3    91   NaN    92    91     89      A+  \n",
       "4    77  83.0    90    92     85      A+  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Scaling on numeric features\n",
    "- One-hot encoding on the categorical feature `major`\n",
    "- Ordinal encoding on the ordinal feature `class_attendance`\n",
    "- Imputation on the `lab2` feature\n",
    "- None on the `ml_experience` feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `ColumnTransformer` example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"quiz2\"])\n",
    "y = df[\"quiz2\"]\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Identify the transformations we want to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [\"university_years\", \"lab1\", \"lab3\", \"lab4\", \"quiz1\"]  # apply scaling\n",
    "categorical_feats = [\"major\"]  # apply one-hot encoding\n",
    "binary_feats = [\"ml_experience\"]  # do not apply any transformation\n",
    "drop_feats = ['lab2', 'class_attendance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's only focus on scaling and one-hot encoding first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Create a column transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each transformation is specified by a name, a transformer object, and the columns this transformer should be applied to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "    [(\"scaling\", StandardScaler(), numeric_feats),\n",
    "     (\"onehot\", OneHotEncoder(sparse=False), categorical_feats),     \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each transformer is applied to the specified columns and the result of the transformations are concatenated horizontally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Convenient `make_column_transformer` syntax\n",
    "\n",
    "- Similar to `make_pipeline` syntax, there is convenient `make_column_transformer` syntax. \n",
    "- The syntax automatically names each step based on its class. \n",
    "- We'll be mostly using this syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (StandardScaler(), numeric_feats), # scaling on numeric features\n",
    "    (OneHotEncoder(), categorical_feats), # OHE on categorical features \n",
    "    (\"passthrough\", binary_feats), # no transformations on the binary features\n",
    "    (\"drop\", drop_feats) # drop the drop features \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "- A big advantage here is that we build all our transformations together into one object, and that way we're sure we do the same operations to all splits of the data.\n",
    "\n",
    "- Otherwise we might, for example, do the OHE on both train and test but forget to scale the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's examine the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = ct.fit_transform(X)\n",
    "type(transformed[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Note that the returned object is not a dataframe. So there are no column names. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Viewing the transformed data as a dataframe\n",
    "\n",
    "- How can we view our transformed data as a dataframe? \n",
    "- We are adding more columns. \n",
    "- So the original columns won't directly to the transformed data. \n",
    "- Let's create column names for the transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "column_names = (\n",
    "    numeric_feats\n",
    "    + ct.named_transformers_[\"onehotencoder\"].get_feature_names().tolist()\n",
    "    + binary_feats\n",
    ")\n",
    "column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Note that the order of the columns in the transformed data depends upon the order of the features we pass to the `ColumnTransformer` and can be different than the order of the features in the original dataframe.  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### `ColumnTransformer`: Transformed data\n",
    "\n",
    "<br>\n",
    "<img src='./img/column-transformer.png' width=\"1500\">\n",
    "\n",
    "[Adapted from here.](https://amueller.github.io/COMS4995-s20/slides/aml-04-preprocessing/#37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training models with transformed data\n",
    "- We can now pass the `ColumnTransformer` object as a step in a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, SVC(gamma=0.001))\n",
    "pipe.fit(X, y)\n",
    "pipe.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the categorical and numeric columns\n",
    "numeric_features = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "    \"rooms_per_household\",\n",
    "    \"bedrooms_per_household\",\n",
    "    \"population_per_household\",\n",
    "]\n",
    "\n",
    "categorical_features = [\"ocean_proximity\"]\n",
    "# reamainder_features = [\"median_income\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's build a pipeline for our dataset\n",
    "- create the preprocessing pipelines for both numeric and categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    # remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we `fit` with the preprocessor, it calls `fit` on _all_ the transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pp = preprocessor.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we transform with the preprocessor, it calls `transform` on _all_ the transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can get the new names of the columns that were generated by the one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names(\n",
    "    categorical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this with the numeric feature names gives us all the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = numeric_features + list(\n",
    "    preprocessor.named_transformers_[\"cat\"]\n",
    "    .named_steps[\"onehot\"]\n",
    "    .get_feature_names(categorical_features)\n",
    ")\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        # (\"reg\", KNeighborsRegressor()),\n",
    "        (\"reg\", SVR(gamma=0.01)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "store_cross_val_results(\"imp + scaling + ohe + SVR\", scores, results_dict)\n",
    "pd.DataFrame(results_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that categorical features are different than free text features. Sometimes there are columns containing free text information and we we'll look at ways to deal with them later in the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `remainder=\"passthrough\"`\n",
    "- Side note: the `ColumnTransformer` will automatically remove columns that are not being transformed:\n",
    "- Use `remainder=\"passthrough\"` of `ColumnTransformer` to keep the other columns in tact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Preprocessing the targets?\n",
    "\n",
    "- Generally no need for this when doing classification. \n",
    "- In regression it makes sense in some cases. More on this in 573. \n",
    "- `sklearn` is fine with categorical labels ($y$-values) for classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"quiz2\", \"lab2\"])\n",
    "y = df[\"quiz2\"]\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_feats = [\"class_attendance\"]  #\n",
    "attendance_cats = [\"Poor\", \"Average\", \"Good\", \"Excellent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (StandardScaler(), numeric_feats),\n",
    "    (OneHotEncoder(), categorical_feats),\n",
    "    # (OrdinalEncoder(categories=[attendance_cats], dtype=int), ordinal_feats),\n",
    "    (\"passthrough\", binary_feats),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat_names.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = ct.fit_transform(X)\n",
    "cat_feat_names = ct.named_transformers_[\"onehotencoder\"].get_feature_names()\n",
    "col_names = numeric_feats + cat_feat_names.tolist() + ordinal_feats + binary_feats\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ct, SVC(gamma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"), StandardScaler()\n",
    ")\n",
    "Ordinal_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    # remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  `ColumnTransformer` <a name=\"7\"></a>\n",
    "- sklearn's [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) makes this more manageable.\n",
    "    - A big advantage here is that we build all our transformations together into one object, and that way we're sure we do the same operations to all splits of the data. \n",
    "    - Otherwise we might, for example, do the OHE on both train and test but forget to scale the test data.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src='./img/column-transformer.png' width=\"1500\">\n",
    "\n",
    "[Adapted from here.](https://amueller.github.io/COMS4995-s20/slides/aml-04-preprocessing/#37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the categorical and numeric columns\n",
    "numeric_features = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "    \"rooms_per_household\",\n",
    "    \"bedrooms_per_household\",\n",
    "    \"population_per_household\",\n",
    "]\n",
    "\n",
    "categorical_features = [\"ocean_proximity\"]\n",
    "# reamainder_features = [\"median_income\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's build a pipeline for our dataset\n",
    "- create the preprocessing pipelines for both numeric and categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    # remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `ColumnTransformer` syntax is somewhat similar to Pipeline in that you pass in a list of tuples.\n",
    "- But here each tuple has 3 values instead of 2: (name, object, list of columns)\n",
    "\n",
    "- A big advantage here is that we build all our transformations together into one object, and that way we're sure we do the same operations to all splits of the data.\n",
    "\n",
    "- Otherwise we might, for example, do the OHE on both train and test but forget to scale the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we `fit` with the preprocessor, it calls `fit` on _all_ the transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pp = preprocessor.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we transform with the preprocessor, it calls `transform` on _all_ the transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can get the new names of the columns that were generated by the one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names(\n",
    "    categorical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this with the numeric feature names gives us all the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = numeric_features + list(\n",
    "    preprocessor.named_transformers_[\"cat\"]\n",
    "    .named_steps[\"onehot\"]\n",
    "    .get_feature_names(categorical_features)\n",
    ")\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        # (\"reg\", KNeighborsRegressor()),\n",
    "        (\"reg\", SVR(gamma=0.01)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "store_cross_val_results(\"imp + scaling + ohe + SVR\", scores, results_dict)\n",
    "pd.DataFrame(results_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that categorical features are different than free text features. Sometimes there are columns containing free text information and we we'll look at ways to deal with them later in the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `remainder=\"passthrough\"`\n",
    "- Side note: the `ColumnTransformer` will automatically remove columns that are not being transformed:\n",
    "- Use `remainder=\"passthrough\"` of `ColumnTransformer` to keep the other columns in tact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Preprocessing the targets?\n",
    "\n",
    "- Generally no need for this when doing classification. \n",
    "- In regression it makes sense in some cases. More on this in 573. \n",
    "- `sklearn` is fine with categorical labels ($y$-values) for classification problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. More on categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data \n",
    "\n",
    "We'll be using [the adult census dataset](https://www.kaggle.com/uciml/adult-census-income#) you used in lab 2. \n",
    "\n",
    "This is a classification dataset and the classification task is to predict whether income exceeds 50K per year or not based on the census data. You can find more information on the dataset and features [here](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "The code below loads the data CSV (assuming that it is saved as `data/adult.csv` in this folder). \n",
    "\n",
    "*Note that many popular datasets have sex as a feature where the possible values are male and female. This representation reflects how the data were collected and is not meant to imply that, for example, gender is binary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "adult_df_large = pd.read_csv(\"data/adult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(adult_df_large, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_nan = train_df.replace(\"?\", np.NaN)\n",
    "test_df_nan = test_df.replace(\"?\", np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_nan[\"income\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the lab we took the simplest approach and and divided the feature in these two categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"age\",\n",
    "    \"fnlwgt\",\n",
    "    \"education.num\",\n",
    "    \"capital.gain\",\n",
    "    \"capital.loss\",\n",
    "    \"hours.per.week\",\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"marital.status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"native.country\",\n",
    "]\n",
    "\n",
    "target = \"income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df_nan.drop(columns=[target])\n",
    "y_train = train_df_nan[target]\n",
    "\n",
    "X_test = test_df_nan.drop(columns=[target])\n",
    "y_test = test_df_nan[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We defined transformations on numeric and categorical features,  \n",
    "- a column transformer, \n",
    "- a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"clf\", SVC()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `make_pipeline` syntax\n",
    "\n",
    "Let's create a column transformer and a pipeline using an alternative syntax `make_pipeline`. \n",
    "\n",
    "- shorthand for the `Pipeline` constructor\n",
    "- does not permit, naming the steps\n",
    "- instead, their names will be set to the lowercase of their types automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(),\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "pipe = make_pipeline(preprocessor, SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `handle_unknown=\"ignore\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe, X_train, y_train, cv=5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- What's going on here??\n",
    "- Let's look at the error message:\n",
    "`Found unknown categories ['Holand-Netherlands'] in column 6 during transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train[\"native.country\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- There is only one instance of Holand-Netherlands.\n",
    "- During cross-validation, this is getting put into the validation split.\n",
    "- By default, `OneHotEncoder` throws an error because you might want to know about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Simplest fix:\n",
    "- Pass `handle_unknown=\"ignore\"` argument to `OneHotEncoder`\n",
    "- It creates a row with all zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "pipe = make_pipeline(preprocessor, SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe, X_train, y_train, cv=5, return_train_score=True)\n",
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Do you want this behaviour? \n",
    "- Are you expecting to get many unknown categories? Do you want to be able to distinguish between them?\n",
    "- With this approach, all unknown categories will be represented with all zeros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cases where it's OK to break the golden rule \n",
    "\n",
    "- If it's some fix number of categories. For example, if it's something like courses in MDS. We know the categories in advance and this is one of the cases where it might be OK to violate the golden rule and get a list of all possible values for the categorical variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A common question that came up in the lab: \n",
    "- What types of features are present in this dataset other than numeric and categorical features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[categorical_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the columns are actually categorical columns, in the sense that there is no ordinality among values. \n",
    "- What about _education_ column? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is actually an order in the values and it might help to encode this column using `OrdinalEncoder`\n",
    "    - Example: Masters > 10th    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"education\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "oe = OrdinalEncoder(dtype=int)\n",
    "oe.fit(X_train[[\"education\"]])\n",
    "ed_transformed = oe.transform(X_train[[\"education\"]])\n",
    "ed_transformed = pd.DataFrame(\n",
    "    data=ed_transformed, columns=[\"education_enc\"], index=X_train.index\n",
    ")\n",
    "ed_transformed.head()\n",
    "oe.categories_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data=np.arange(len(oe.categories_[0])),\n",
    "    columns=[\"transformed\"],\n",
    "    index=oe.categories_[0],\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `OrdinalEncoder` has encoded the categories by alphabetically sorting them and then assigning integers to them in that order.\n",
    "- Is this what we want? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"education\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's order them manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_levels = [\n",
    "    \"Preschool\",\n",
    "    \"1st-4th\",\n",
    "    \"5th-6th\",\n",
    "    \"7th-8th\",\n",
    "    \"9th\",\n",
    "    \"10th\",\n",
    "    \"11th\",\n",
    "    \"12th\",\n",
    "    \"HS-grad\",\n",
    "    \"Prof-school\",\n",
    "    \"Assoc-voc\",\n",
    "    \"Assoc-acdm\",\n",
    "    \"Some-college\",\n",
    "    \"Bachelors\",\n",
    "    \"Masters\",\n",
    "    \"Doctorate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(education_levels) == set(train_df[\"education\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe = OrdinalEncoder(categories=[education_levels], dtype=int)\n",
    "oe.fit(X_train[[\"education\"]])\n",
    "ed_transformed = oe.transform(X_train[[\"education\"]])\n",
    "ed_transformed = pd.DataFrame(\n",
    "    data=ed_transformed, columns=[\"education_enc\"], index=X_train.index\n",
    ")\n",
    "oe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"age\", \"fnlwgt\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "categorical_features = [\n",
    "    \"workclass\",\n",
    "    \"marital.status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"native.country\",\n",
    "]\n",
    "ordinal_features = [\"education\"]\n",
    "target_column = \"income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    ")\n",
    "\n",
    "ordinal_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OrdinalEncoder(\n",
    "        categories=[education_levels],\n",
    "        dtype=int,\n",
    "    ),\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "        (\"ordinal\", ordinal_transformer, ordinal_features),\n",
    "    ]\n",
    ")\n",
    "pipe = make_pipeline(preprocessor, SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Binary features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this dataset the only feature coded with two possible values in the dataset is sex.\n",
    "- Let's try OHE on that feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"age\", \"fnlwgt\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "categorical_features = [\n",
    "    \"workclass\",\n",
    "    \"marital.status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"native.country\",\n",
    "]\n",
    "ordinal_features = [\"education\"]\n",
    "binary_features = [\"sex\"]\n",
    "target_column = \"income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    ")\n",
    "\n",
    "ordinal_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OrdinalEncoder(\n",
    "        categories=[education_levels],\n",
    "        dtype=int,\n",
    "    ),\n",
    ")\n",
    "\n",
    "binary_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(drop=\"if_binary\", dtype=int),\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "        (\"ordinal\", ordinal_transformer, ordinal_features),\n",
    "        (\"binary\", binary_transformer, binary_features),\n",
    "    ]\n",
    ")\n",
    "pipe = make_pipeline(preprocessor, SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_validate(pipe, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OHE with many categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"native.country\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Do we have enough data for rare categories to learn anything meaningful? \n",
    "- How about grouping them into bigger categories\n",
    "    - Example: \"South America\" or \"Asia\"\n",
    "- Or having \"other\" category for rare cases? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Do we actually want to use certain features for prediction?\n",
    "\n",
    "- Do you want to use `race` in prediction?\n",
    "- Remember that the systems you build are going to be used in some applications. \n",
    "- It's extremely important to be mindful of the consequences of including certain features in your predictive model. \n",
    "- I would just drop the feature to avoid racial biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorical features (True or False)\n",
    "\n",
    "- `handle_unknown=\"ignore\"` would treat all unknown categories equally. \n",
    "- Creating groups of rarely occurring categories might overfit the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Encoding text data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- ML algorithms we have seen so far prefer numeric and fixed length input that looks like this: \n",
    "\n",
    "$$X = \\begin{bmatrix}1.0 & 4.0 & \\ldots & & 3.0\\\\ 0.0 & 2.0 & \\ldots & & 6.0\\\\ 1.0 & 0.0 & \\ldots & & 0.0\\\\ \\end{bmatrix}$$ \n",
    "\n",
    "and \n",
    "$$y = \\begin{bmatrix}spam \\\\ non spam \\\\ spam \\end{bmatrix}$$\n",
    "\n",
    "- But what if we are only given data in the form of raw text and associated labels?\n",
    "- How can we represent such data into fixed number of features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spam/non-spam toy example\n",
    "\n",
    "Would you be able to apply the algorithms we have seen so far on the data that looks like this?\n",
    "\n",
    "$X = \\begin{bmatrix}\\text{\"URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!\",}\\\\ \\text{\"Lol your always so convincing.\"}\\\\ \\text{\"Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now!\"}\\\\ \\end{bmatrix}$ \n",
    "\n",
    "and \n",
    "\n",
    "$y = \\begin{bmatrix}spam \\\\ non spam \\\\ spam \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In categorical features or ordinal features, we have fixed number of categories.\n",
    "- In text features such as above, each feature value (i.e., each text message) is going to be different. \n",
    "- How do we encode these feature? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bag of words (BOW) representation\n",
    "\n",
    "- One way is to use a simple bag of words (BOW) representation which involves two components. \n",
    "    - The vocabulary (all unique words in all documents) \n",
    "    - A value indicating either the presence or absence or the count of each word in the document. \n",
    "        \n",
    "<center>\n",
    "<img src='./img/bag-of-words.png' width=\"800\">\n",
    "</center>\n",
    "\n",
    "[Source](https://web.stanford.edu/~jurafsky/slp3/4.pdf)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extracting BOW features using `scikit-learn`\n",
    "- `CountVectorizer`\n",
    "    - Converts a collection of text documents to a matrix of word counts.  \n",
    "    - Each row represents a \"document\" (e.g., a text message in our example). \n",
    "    - Each column represents a word in the vocabulary in the training data. \n",
    "    - Each cell represents how often the word occurs in the document. \n",
    "    \n",
    "    \n",
    "Note: In the NLP community a text data set is referred to as a **corpus** (plural: corpora).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = [\n",
    "    \"URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!\",\n",
    "    \"Lol you are always so convincing.\",\n",
    "    \"Nah I don't think he goes to usf, he lives around here though\",\n",
    "    \"URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!\",\n",
    "    \"Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\",\n",
    "    \"As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\",\n",
    "]\n",
    "y = [\"spam\", \"non spam\", \"non spam\", \"spam\", \"spam\", \"non spam\"]\n",
    "vec = CountVectorizer()\n",
    "X_counts = vec.fit_transform(X)\n",
    "bow_df = pd.DataFrame(X_counts.toarray(), columns=sorted(vec.vocabulary_), index=X)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of elements: \", np.prod(X_counts.shape))\n",
    "print(\"The number of non-zero elements: \", X_counts.nnz)\n",
    "print(\n",
    "    \"Proportion of non-zero elements: %0.4f\" % (X_counts.nnz / np.prod(X_counts.shape))\n",
    ")\n",
    "print(\n",
    "    \"The value at cell 3,%d is: %d\"\n",
    "    % (vec.vocabulary_[\"jackpot\"], X_counts[3, vec.vocabulary_[\"jackpot\"]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why sparse matrices? \n",
    "\n",
    "- Most words do not appear in a given document.\n",
    "- We get massive computational savings if we only store the nonzero elements.\n",
    "- There is a bit of overhead, because we also need to store the locations:\n",
    "    - e.g. \"location (3,31): 1\".\n",
    "    \n",
    "- However, if the fraction of nonzero is small, this is a huge win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Question for you\n",
    "- What would happen if you apply `StandardScaler` on sparse data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### `OneHotEncoder` and sparse features \n",
    "- By default, `OneHotEncoder` also creates sparse features. \n",
    "- You could set `sparse=False` to get a regular `numpy` array. \n",
    "- If there are a huge number of categories, it may be beneficial to keep them sparse.\n",
    "- For smaller number of categories, it doesn't matter much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important hyperparameters of `CountVectorizer` \n",
    "\n",
    "- `binary`\n",
    "    - whether to use absence/presence feature values or counts\n",
    "- `max_features`\n",
    "    - only consider top `max_features` ordered by frequency in the corpus\n",
    "- `max_df`\n",
    "    - ignore features which occur in more than `max_df` documents \n",
    "- `min_df` \n",
    "    - ignore features which occur in less than `min_df` documents \n",
    "- `ngram_range`\n",
    "    - consider word sequences in the given range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at all features, i.e., words (along with their frequencies).\n",
    "vec_all = CountVectorizer()\n",
    "X_counts = vec_all.fit_transform(X)\n",
    "pd.DataFrame(\n",
    "    data=X_counts.sum(axis=0).tolist()[0],\n",
    "    index=vec_all.get_feature_names(),\n",
    "    columns=[\"counts\"],\n",
    ").sort_values(\"counts\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can control the size of X (the number of features) using `max_features`\n",
    "vec8 = CountVectorizer(max_features=8)\n",
    "X_counts = vec8.fit_transform(X)\n",
    "pd.DataFrame(\n",
    "    data=X_counts.sum(axis=0).tolist()[0],\n",
    "    index=vec8.get_feature_names(),\n",
    "    columns=[\"counts\"],\n",
    ").sort_values(\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bow_df = pd.DataFrame(X_counts.toarray(), columns=sorted(vec8.vocabulary_), index=X)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vec8_binary = CountVectorizer(binary=True, max_features=8)\n",
    "X_counts = vec8_binary.fit_transform(X)\n",
    "pd.DataFrame(\n",
    "    data=X_counts.sum(axis=0).tolist()[0],\n",
    "    index=vec8.get_feature_names(),\n",
    "    columns=[\"counts\"],\n",
    ").sort_values(\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that `vec8` and `vec8_binary` have different vocabularies, which is kind of unexpected behaviour and doesn't match the documentation of `scikit-learn`. \n",
    "\n",
    "[Here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L1206-L1225) is the code for `binary=True` condition in `scikit-learn`. As we can see, the binarization is done before limiting the features to `max_features`, and so now we are actually looking at the document counts (in how many documents it occurs) rather than term count. This is not explained anywhere in the documentation. \n",
    "\n",
    "The ties in counts between different words makes it even more confusing. I don't think it'll have a big impact on the results but this is good to know! Remember that `scikit-learn` developers are also humans who are prone to make mistakes. So it's always a good habit to question whatever tools we use every now and then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df = pd.DataFrame(\n",
    "    X_counts.toarray(), columns=sorted(vec8_binary.vocabulary_), index=X\n",
    ")\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Note that `CountVectorizer` is carrying out some preprocessing such as because of the default argument values \n",
    "    - Converting words to lowercase (`lowercase=True`)\n",
    "    - getting rid of punctuation and special characters (`token_pattern ='(?u)\\\\b\\\\w\\\\w+\\\\b'`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(CountVectorizer(), SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is this a realistic representation of text data? \n",
    "\n",
    "- Of course this is not a great representation of language\n",
    "    - We are throwing out everything we know about language and losing a lot of information. \n",
    "    - It assumes that there is no syntax and compositional meaning in language.  \n",
    "- But it works surprisingly well for many tasks. \n",
    "- We will learn more expressive representations later in the program in DSCI 575 (my favorite course :))! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lab you'll develop a system for spam identification on a dataset from Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CountVectorizer`: True or False\n",
    "\n",
    "- As you increase the value for `max_features` hyperparameter of `CountVectorizer` the training score is likely to go up. \n",
    "    - Varada's answer: True because increasing the value of `max_features` means we include each and every word from the training data in the dictionary and the training score is likely to go up. \n",
    "- If we encounter a word in the validation or the test split that's not available in the training data, we'll get an error. \n",
    "    - Varada's answer: False because if the word isn't in the dictionary, we would just ignore the word. \n",
    "- `max_df` hyperparameter of `CountVectorizer` can be used to get rid of most frequently occurring words from the dictionary.    \n",
    "    - True because words such as _a_, _the_, _in_, _of_ occur in most of the documents, and with `max_df` hyperparameter, we can control the features to be used based on the number of documents they occur in. So if we set this to a higher proportion, we can get rid of such stop words.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_df = pd.read_csv(\"data/cleaned_restaurant_data.csv\")\n",
    "restaurant_subset = restaurant_df[[\"n_people\", \"price\", \"target\"]]\n",
    "clean_restaurant_subset = restaurant_subset[restaurant_df[\"price\"] < 200]\n",
    "clean_restaurant_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_restaurant_subset.drop(columns=[\"target\"])\n",
    "y = clean_restaurant_subset[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=4)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"code/.\")\n",
    "import graphviz\n",
    "import IPython\n",
    "import mglearn\n",
    "from IPython.display import HTML, display\n",
    "from plotting_functions import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=5)\n",
    "model.fit(X_train, y_train)\n",
    "plot_tree_decision_boundary_and_tree(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    height=6,\n",
    "    width=16,\n",
    "    eps=10,\n",
    "    x_label=\"n_people\",\n",
    "    y_label=\"price\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
