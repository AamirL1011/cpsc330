{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 hw4\n",
    "\n",
    "Meta-commentary: this assignment contains more new material that I created specifically for CPSC 330, whereas previous assignments relied more on adapting materials from other courses. As a result, in this assignment it is more likely that we will encounter typos, bugs, or other frustrations. Please be patient as we work through these issues.\n",
    "\n",
    "Following the style of the lectures, this assignment is centred around a particular dataset and is also somewhat more open-ended than the previous ones. This reflects the direction that I'm trying to take the course, namely to give you practice with, and build good habits for, the end-to-end ML process. However, if this turns out to be too messy or too difficult to grade then I may revert back to less open-ended assignments in the future. Note that, given this style, there are many possible correct answers - you should not expect exactly the same results as your classmates.\n",
    "\n",
    "It is also a bit hard for me to gauge the right length and difficulty for a new assignment, so please do provide feedback in your repo's README!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={points:5}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.students.cs.ubc.ca/cpsc330-2019w-t2/home/blob/master/docs/homework_instructions.md). \n",
    "\n",
    "**Additional requirement**: if you are working with a partner, please write a couple sentences explaining the contribution of each team member. You should refer to yourselves by your CSIDs (because seeing names can cause bias during grading). Here is an example:\n",
    "\n",
    "> a1b2c did Exercise 1, checked over Exercise 2, and pair-programmed for Exercise 3. z9y8x checked over Exercise 1, did Exercise 2, and pair-programmed for Exercise 3. \n",
    "\n",
    "Our ideal scenario is that you worked together on all the exercises, but you are not required to do so, and for now we are only collecting this information because we are curious. If you are working alone, you can ignore this section.\n",
    "\n",
    "_YOUR TEAMWORK CONTRIBUTION STATEMENT GOES HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing quality/quantity\n",
    "rubric={points:5}\n",
    "\n",
    "The TAs have reported a couple issues with the first few assignments: in some cases, submissions simply show the code output with no commentary; please write at least a sentence explaining your output in each question. In other cases, the TAs have come across multi-paragraph answers where a couple of sentences would have sufficed. Thus, we are now allocating the above points for well-structured answers of a reasonable length. In general, 1-3 sentences is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: implementing `DummyClassifier`\n",
    "rubric={points:20}\n",
    "\n",
    "In this course (unlike CPSC 340) you will generally not be asked to implement the methods we talk about, like logistic regression. However, this exercise is an exception: you will implement the simplest possible classifier, `DummyClassifier`.\n",
    "\n",
    "Below you will find starter code for a class called `MyDummyClassifier`, which has methods `fit()`, `predict()`, and `predict_proba()`. Your task is to fill in those three functions. The next code block has some tests you can use to assess whether your code is working. \n",
    "\n",
    "I suggest starting with `fit` and `predict`, and making sure those are working before moving on to `predict_proba`. For `predict_proba`, you should return the frequency of each class in the training data. Again, you can compare with `DummyClassifier` using the code below.\n",
    "\n",
    "To simplify this question, you can assume **binary classification**, and furthermore that these classes are **encoded as 0 and 1**. In other words, you can assume that `y` contains only 0s and 1s. The real `DummyClassifier` works when you have more than two classes, and also works if the target values are encoded differently, for example as \"cat\", \"dog\", \"mouse\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDummyClassifier:\n",
    "    \"\"\"\n",
    "    A baseline classifier that predicts the most common class.\n",
    "    The predicted probabilities come from the relative frequencies\n",
    "    of the classes in the training data.\n",
    "    \n",
    "    This implementation only works when y only contains 0s and 1s.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        pass # your code here\n",
    "    \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        pass # your code here\n",
    "    \n",
    "        \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        pass # your code here\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some tests for `predict`. You may want to run the cell a few times to make sure you explore the different cases (or automate this with a loop or random seeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, generate random data\n",
    "n_train = 101\n",
    "n_valid = 21\n",
    "d = 5\n",
    "X_train_dummy = np.random.randn(n_train, d)\n",
    "X_valid_dummy = np.random.randn(n_valid, d)\n",
    "y_train_dummy = np.random.randint(2, size=n_train)\n",
    "y_valid_dummy = np.random.randint(2, size=n_valid)\n",
    "\n",
    "my_dc = MyDummyClassifier()\n",
    "sk_dc = DummyClassifier(strategy=\"prior\")\n",
    "\n",
    "my_dc.fit(X_train_dummy, y_train_dummy);\n",
    "sk_dc.fit(X_train_dummy, y_train_dummy);\n",
    "\n",
    "assert np.array_equal(my_dc.predict(X_train_dummy), sk_dc.predict(X_train_dummy))\n",
    "assert np.array_equal(my_dc.predict(X_valid_dummy), sk_dc.predict(X_valid_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some tests for `predict_proba`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(my_dc.predict_proba(X_train_dummy), sk_dc.predict_proba(X_train_dummy))\n",
    "assert np.array_equal(my_dc.predict_proba(X_valid_dummy), sk_dc.predict_proba(X_valid_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Precision and recall by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the confusion matrix of a machine learning system that predicts whether a cancer is malignant or not. Let's consider malignant to be the \"positive class\".\n",
    "\n",
    "|    Actual/Predicted      | Predicted Benign | Predicted Malignant |\n",
    "| :------------- | -----------------------: | -----------------------: |\n",
    "| **Actual Benign**       | 6 | 238 |\n",
    "| **Actual Malignant**       | 20 | 194 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(a)\n",
    "rubric={points:2}\n",
    "\n",
    "Would you consider this an imbalanced dataset? Why or why not? Max 2 sentences.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(b)\n",
    "rubric={points:2}\n",
    "\n",
    "Based on the above confusion matrix, what is the recall? \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(c)\n",
    "rubric={points:5}\n",
    "\n",
    "Do you consider this to be a good classifier? What additional information might you need to answer this question? Briefly discuss in 1-3 sentences.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Customer churn data\n",
    "\n",
    "[Customer churn](https://en.wikipedia.org/wiki/Customer_attrition) refers to the notion of customers leaving a subscription service like Netflix. In this exercise, we will try to predict customer churn in a dataset where most of the customers stay with the service and a small minority cancel their subscription. To start, please download the [Kaggle telecom customer churn dataset](https://www.kaggle.com/becksddf/churn-in-telecoms-dataset). **Do not push the CSV to your repo** (you may want to create a .gitignore file). One you have the data, you should be able to run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bigml_59c28831336c6604c800002a.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=100)\n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.25, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column (`churn`) is the target. \"True\" means the customer left the subscription (churned) and \"False\" means they stayed.\n",
    "\n",
    "**Note**: in this exercise you are welcome to copy/paste/adapt code from the **lecture notes** without attribution. However, you are **not** permitted to copy any other code from online sources without attribution.\n",
    "\n",
    "**Note**: if available, you are welcome to use scikit-learn functions for any of the tasks below, such as confusion matrix. You are not required to implement them yourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(a)  \n",
    "rubric={points:8} \n",
    "\n",
    "Perform some exploratory data analysis on the training set. In particular:\n",
    "\n",
    "- How many rows and columns are there?\n",
    "- How many True/False target values are there?\n",
    "\n",
    "Come up with **two** more questions you would like to answer (similar to the above two), and explore those as well. Briefly discuss your results in 1-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(b)\n",
    "rubric={points:20}\n",
    "\n",
    "In preparation for building a classifier, perform whatever feature transformations you deem sensible. Use `ColumnTransformer` to combine the transformations (see Lecture 6 or 8). This can include dropping features if you think they are not helpful. \n",
    "\n",
    "In each case, briefly explain your rationale with 1-2 sentences. You do not need an explanation for every feature, but for every group of features that are being transformed the same way. For example, \"I am doing transformation X to the following categorical features: `a`, `b`, `c` because of reason Y,\" etc.\n",
    "\n",
    "Warning: as discussed in lecture, make sure not to violate the golden rule; you should not be calling `fit` or `fit_transform` on any test data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(c)\n",
    "rubric={points:10}\n",
    "\n",
    "\"Train\" a `DummyClassifier` on your transformed data, using `strategy='prior'` as in Exercise 1. Report the following:\n",
    "\n",
    "1. Train and validation accuracy.\n",
    "2. Confusion matrix on the validation data.\n",
    "3. Precision, recall, F1-score on the validation data.\n",
    "\n",
    "Briefly comment on your results (2 sentences max)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(d)\n",
    "rubric={points:20} \n",
    "\n",
    "Train a logistic regression classifier on your transformed data, using the default hyperparameters. Report the following metrics:\n",
    "\n",
    "1. Train and validation accuracy.\n",
    "2. Confusion matrix on the validation data.\n",
    "3. Precision, recall, F1-score on the validation data.\n",
    "\n",
    "Are you satisfied with the results? Use your `DummyClassifier` results as a reference point. Briefly discuss (1 paragraph max). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(e)\n",
    "rubric={points:5}\n",
    "\n",
    "Set the `class_weight` parameter of your logistic regression model to `'balanced'`. Report the following metrics:\n",
    "\n",
    "1. Train and validation accuracy.\n",
    "2. Confusion matrix on the validation data.\n",
    "3. Precision, recall, F1-score on the validation data.\n",
    "\n",
    "Discuss your results in 1-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(f)\n",
    "rubric={points:5}\n",
    "\n",
    "On the same axes, plot the ROC curves for the three methods we tried. Make sure you have a legend labeling which curve is which. Also, report the AUC in each case. Briefly comment on your results (1 sentence).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(g)\n",
    "rubric={points:10}\n",
    "\n",
    "The function below plots histograms of the predicted probability, split by the true class, for each of the two logistic regression models. These are similar to the animated plots from lecture. \n",
    "\n",
    "Call this function using your (transformed) **validation** data and your two logistic regression models. Then, discuss your results. How did the regular and balanced logistic regression models compare in terms of accuracy and recall? How did the two models compare in terms of ROC curves? Do these new plots help explain what is going on here? Max 1 paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hists(X, y, lr_original, lr_balanced):\n",
    "\n",
    "    negative_examples = X_valid[y_valid == 0]\n",
    "    positive_examples = X_valid[y_valid == 1]\n",
    "\n",
    "    for name, model in {\"log reg\" : lr, \"log reg balanced\" :lr_balanced}.items():\n",
    "\n",
    "        plt.hist(model.predict_proba(negative_examples)[:,1], alpha=0.5, bins=30, label=\"0\", density=True)\n",
    "        plt.hist(model.predict_proba(positive_examples)[:,1], alpha=0.5, bins=30, label=\"1\", density=True)\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        plt.xlabel(\"predicted probability\")\n",
    "        plt.ylabel(\"normalized counts\")\n",
    "        plt.title(name);\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4(a)\n",
    "rubric={points:5}\n",
    "\n",
    "Try applying a random forest to this problem, again using `class_weight='balanced'`. Report the following metrics:\n",
    "\n",
    "1. Train and validation accuracy.\n",
    "2. Confusion matrix on the validation data.\n",
    "3. Precision, recall, F1-score on the validation data. \n",
    "\n",
    "Briefly comment on the results (max 2 sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4(b)\n",
    "rubric={points:5}\n",
    "\n",
    "Next we will optimize the `n_estimators` hyperparameter of your random forest using `RandomizedSearchCV`, keeping `class_weight='balanced'`. Because cross-validation separates the folds for us, I will combine the training and validation sets so that we have more data to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid = np.concatenate((X_train, X_valid), axis=0)\n",
    "y_train_valid = np.concatenate((y_train, y_valid), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "param_dist = {\n",
    "              \"n_estimators\"     : scipy.stats.randint(low=10, high=300),\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=321), \n",
    "                                   param_distributions = param_dist, \n",
    "                                   n_iter = 20, \n",
    "                                   cv=3,\n",
    "                                   verbose=1, \n",
    "                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(X_train_valid, y_train_valid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is incredible - it gets 100% validation accuracy! This means it make no errors, so it must also be getting a perfect precision, recall, and F1-score.\n",
    "What is wrong with my analysis? Answer in 1-2 sentences.\n",
    "\n",
    "(Note: when you run the above code you might get slightly different results, depending on your feature preprocessing in Exercise 3. That is fine. Hopefully the score isn't too different.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4(c)\n",
    "rubric={points:5}\n",
    "\n",
    "Repeat the hyperparameter optimization from the previous part, this time doing it correctly. For your optimized model, report the F1-score on the validation set. Briefly comment on the results (max 2 sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4(d)\n",
    "rubric={points:5}\n",
    "\n",
    "This time optimize both `n_estimators` and `max_depth`. What F1-score do you get on the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4(e) \n",
    "rubric={points:5}\n",
    "\n",
    "When you carry out hyperparameter optimization, by default it is maximizing accuracy. In unbalanced datasets such as churn datasets, using accuracy does not make sense. You can use different [scoring metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) when you carry out hyperparameter optimization using `GridSearchCV` or `RandomizedSearchCV` using the `scoring` parameter. Try optimizing your model to pick the hyerparameters with the best F1-score by setting `scoring='f1'` when creating the `RandomizedSearchCV`. What F1-score do you achieve on the validation set?\n",
    "\n",
    "Optional note / FYI: in the case of random search with a fixed random seed, you will end up exploring the same hyperparameter values as in the previous part. The only difference will be which one you consider the best. However, if you were using a fancier method like Bayesian optimization, then the choice of scoring function would actually affect which hyperparameter values were explored, because the suggested next set of hyperparameters depends on the scores of the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4(f)\n",
    "rubric={points:5}\n",
    "\n",
    "Evaluate your final model on the test data. Briefly discuss your results (1-2 sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "_merged",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
