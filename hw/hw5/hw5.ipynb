{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 hw5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we'll be exploring a [dataset](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) of New York City Airbnb listings from 2019. As usually, you'll need to start by downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={points:5}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.students.cs.ubc.ca/cpsc330-2019w-t2/home/blob/master/docs/homework_instructions.md). \n",
    "\n",
    "**Additional requirement**: if you are working with a partner, please write a couple sentences explaining the contribution of each team member. You should refer to yourselves by your CSIDs (because seeing names can cause bias during grading). Here is an example:\n",
    "\n",
    "> a1b2c did Exercise 1, checked over Exercise 2, and pair-programmed for Exercise 3. z9y8x checked over Exercise 1, did Exercise 2, and pair-programmed for Exercise 3. \n",
    "\n",
    "Our ideal scenario is that you worked together on all the exercises, but you are not required to do so, and for now we are only collecting this information because we are curious. If you are working alone, you can ignore this section.\n",
    "\n",
    "_YOUR TEAMWORK CONTRIBUTION STATEMENT GOES HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing quality/quantity\n",
    "rubric={points:5}\n",
    "\n",
    "The TAs have reported a couple issues with the first few assignments: in some cases, submissions simply show the code output with no commentary; please write at least a sentence explaining your output in each question. In other cases, the TAs have come across multi-paragraph answers where a couple of sentences would have sufficed. Thus, we are now allocating the above points for well-structured answers of a reasonable length. In general, 1-3 sentences is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AB_NYC_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=123)\n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we'll try to predict `reviews_per_month`, as a proxy for the popularity of the listing. Airbnb could use this sort of model to predict how popular future listings might be before they are posted, perhaps to help guide hosts create more appealing listings. In reality they might instead use something like vacancy rate or average rating as their target, but we do not have that available here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'reviews_per_month'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[target_name]\n",
    "y_valid = df_valid[target_name]\n",
    "y_test  = df_test[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that listings with 0 reviews have `reviews_per_month` set to NaN instead of 0. I will fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.fillna(value=0)\n",
    "y_valid = y_valid.fillna(value=0)\n",
    "y_test  = y_test.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['neighbourhood', 'neighbourhood_group', 'room_type']\n",
    "drop_features        = ['id', 'name', 'host_id', 'host_name', 'last_review']\n",
    "numeric_features     = ['latitude', 'longitude', 'price', 'minimum_nights', 'number_of_reviews', 'calculated_host_listings_count', 'availability_365']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all columns are accounted for (this is better than what I was doing previous with sets, because it also finds duplicates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sorted(df.columns) == sorted(categorical_features + drop_features + numeric_features + [target_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers=[\n",
    "    ('scale',  StandardScaler(), numeric_features),\n",
    "    ('ohe',    OneHotEncoder(drop='first', sparse=False), categorical_features[1:]),\n",
    "    ('ohe-nodrop', OneHotEncoder(handle_unknown='ignore', sparse=False), categorical_features[:1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = numeric_features + list(preprocessor.named_transformers_['ohe'].get_feature_names(categorical_features[1:])) + list(preprocessor.named_transformers_['ohe-nodrop'].get_feature_names(categorical_features[:1]))\n",
    "new_columns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(preprocessor.transform(df_train), index=df_train.index, columns=new_columns)\n",
    "X_valid = pd.DataFrame(preprocessor.transform(df_valid), index=df_valid.index, columns=new_columns)\n",
    "X_test  = pd.DataFrame(preprocessor.transform(df_test),  index=df_test.index,  columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll try training some models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = DummyRegressor()\n",
    "dr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the score here is the $R^2$. Let's try linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=100)\n",
    "lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coefs = pd.DataFrame(data=lr.coef_, index=X_train.columns, columns=[\"Coefficient\"])\n",
    "lr_coefs.sort_values(by=\"Coefficient\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(a)\n",
    "rubric={points:5}\n",
    "\n",
    "Looking at the top coefficients of the linear regression model reveals an issue with the way we set up the data. One of those features should not be used in the analysis. \n",
    "\n",
    "- Explain the issue.\n",
    "- Fix the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(b)\n",
    "rubric={points:10}\n",
    "\n",
    "In my data preparation, I set `handle_unknown='ignore'` in the `OneHotEncoder` for the `neighbourhood` feature, but not the `neighbourhood_group` feature. \n",
    "\n",
    "- Why was this necessary for `neighbourhood`? \n",
    "- Why was this **not** necessary for `neighbourhood_group`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(c)\n",
    "rubric={points:10}\n",
    "\n",
    "Perhaps it would have been reasonable to take all neighbourhoods across all 3 splits (train/valid/test) and use those to train the `OneHotEncoder` for `neighbourhood`. In general it's a violation of the Golden Rule to look at the validation/test data in our feature preprocessing. \n",
    "\n",
    "- Make an argument for why it might be OK in this case.\n",
    "- But explain what might go wrong if we did it this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(d)\n",
    "rubric={points:10}\n",
    "\n",
    "Earlier we dropped the `name` column, but perhaps the names contain useful information. For example, maybe listings with the word \"new\" are more popular than listings with the word \"rustic\". Let's update the feature preprocessing so that `name` is not dropped, but instead encoded with `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features=100, stop_words='english')\n",
    "count_vec.fit(df_train['name']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain the issue.\n",
    "- Modify `df_train`, `df_valid`, and `df_test` to fix the issue, so that the same line of code (above) runs successfully (below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec.fit(df_train['name']); # this should work now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the issue is fixed, we'll proceed to transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_feature_names = ['name_' + word for word in count_vec.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_name = pd.DataFrame(data=count_vec.transform(df_train[\"name\"]).toarray(), columns=word_feature_names, index=X_train.index)\n",
    "X_valid_name = pd.DataFrame(data=count_vec.transform(df_valid[\"name\"]).toarray(), columns=word_feature_names, index=X_valid.index)\n",
    "X_test_name  = pd.DataFrame(data=count_vec.transform(df_test[\"name\"]).toarray(),  columns=word_feature_names, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words = pd.concat((X_train, X_train_name), axis=1)\n",
    "X_valid_words = pd.concat((X_valid, X_valid_name), axis=1)\n",
    "X_test_words  = pd.concat((X_test, X_test_name),   axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a lot of features generated by the OHE and the word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words.columns[X_train_words.columns.str.startswith(\"neighbourhood\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words.columns[X_train_words.columns.str.startswith(\"name\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, it seems we now have a feature called `neighbourhood_Williamsburg` and a feature called `name_williamsburg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words[['neighbourhood_Williamsburg', 'name_williamsburg']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(e)\n",
    "rubric={points:5}\n",
    "\n",
    "- Would it make sense to remove the feature `neighbourhood_Williamsburg`? Briefly explain.\n",
    "- Would it make sense to remove the feature `name_williamsburg`? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we explore the target values. The number of reviews per month ranges quite a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to log-transform these targets, because decide we care more about relative than absolute error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log(y_train)\n",
    "y_valid_log = np.log(y_valid)\n",
    "y_test_log  = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(f)\n",
    "rubric={points:5}\n",
    "\n",
    "- Explain the issue.\n",
    "- Fix the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: model selection\n",
    "rubric={points:15}\n",
    "\n",
    "Spend 5-20 minutes tuning a `Ridge` and a `RandomForestRegressor` on this problem, using `X_*_words` and `y_*_log` (where `*` is a split name). Which model do you think is better here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: feature importances\n",
    "\n",
    "For this exercise we'll consider the random forest created below, which performs somewhat decently (though still not great):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(max_depth=20, max_features=20, n_estimators=20, random_state=20)\n",
    "rf.fit(X_train_words, y_train_log);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_train_words, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_valid_words, y_valid_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(a)\n",
    "rubric={points:10}\n",
    "\n",
    "Look at the feature importances for this random forest. What features seem to be driving your predictions most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(b)\n",
    "rubric={points:5}\n",
    "\n",
    "For the two most important features in the above model, do you think increasing these features increases or decreases the predicted number of reviews per month? Briefly justify your answer. **Maximum 2 sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(c)\n",
    "rubric={points:10}\n",
    "\n",
    "Use SHAP to try to answer the above question, for predictions on the training set. You will likely need to take a subset of the training examples for speed when computing the SHAP values. **Maximum 1 paragraph.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code adds a column of random noise to `X` and re-trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words_noise = pd.concat((X_train_words, pd.DataFrame(np.random.randn(X_train_words.shape[0],1), columns=['noise'], index=X_train_words.index)), axis=1)\n",
    "X_valid_words_noise = pd.concat((X_valid_words, pd.DataFrame(np.random.randn(X_valid_words.shape[0],1), columns=['noise'], index=X_valid_words.index)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_noise = RandomForestRegressor(max_depth=20, max_features=20, n_estimators=20, random_state=20)\n",
    "rf_noise.fit(X_train_words_noise, y_train_log);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can look at the feature importances of this noise column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=rf_noise.feature_importances_, index=X_train_words_noise.columns, columns=[\"Importance\"]).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(d)\n",
    "rubric={points:5}\n",
    "\n",
    "Why is the importance of the random noise feature non-zero, and in fact larger than that of other features? **Maximum 2 sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(e)\n",
    "rubric={points:15}\n",
    "\n",
    "One of the numeric features is `price`, which is presumably in U.S. dollars per night. Using a `Ridge` model, look at the coefficient corresponding to `price`. Find the relationship implied by the model, in the units of (reviews per month) / (dollars per night). \n",
    "\n",
    "Hint: should you use the log-transformed target values or the originals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(f)\n",
    "rubric={points:10}\n",
    "\n",
    "Perform an open-ended exploration of the feature importances using the model of your choosing. What is the most fun or interesting result you found? You could examine, for example:\n",
    "\n",
    "- Different room types\n",
    "- The importance of various numeric features\n",
    "- The \"most positive\" and \"most negative\" words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(g)\n",
    "rubric={points:10}\n",
    "\n",
    "Given your exploration in the previous part, can you draw conclusions about the world, such as the desireability of various neighbourhoods in New York? Could you now give advice to a friend on what words are best to use in an Airbnb listing? My answer, as you may have guessed, is no! (Or, at least, not without extreme caution and/or statistical training that goes beyond the scope of this course.) Give **3 reasons** why you might be wrong if you drew such conclusions about the world from the above analysis. **Maximum 1 sentence** per reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: very short answer questions\n",
    "rubric={points:40}\n",
    "\n",
    "Answer each of the following questions in **at most 1 sentence**. Each one is worth 5 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If `neighbourhood_group` is aready a feature, why might `neighbourhood` still be worth including?\n",
    "2. If `neighbourhood` is aready a feature, why might `neighbourhood_group` still be worth including?\n",
    "3. If you had an enormous training set and could only keep one of `neighbourhood` or `neighbourhood_group`, which one would you keep, and why?\n",
    "4. How does the size of your training set influence how much you \"trust\" your feature importances?\n",
    "5. If you wanted to find the \"most positive\" and \"most negative\" words in terms of reviews per month, would you use `Ridge` or `RandomForestRegressor`? Assume you're using only scikit-learn (no SHAP, etc.).\n",
    "6. What is an advantage of ensembling multiple models as opposed to just choosing one of them?\n",
    "7. What is an disadvantage of ensembling multiple models as opposed to just choosing one of them?\n",
    "8. By default, `StackingRegressor` uses `Ridge` as its \"meta-model\". Explain the significance of the coefficients learned by this `Ridge` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "_merged",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
