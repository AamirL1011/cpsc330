{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 hw6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `nltk` and `gensim`, see Lecture 13 for install instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={points:5}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.students.cs.ubc.ca/cpsc330-2019w-t2/home/blob/master/docs/homework_instructions.md). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "In this assignment we'll be looking at a [dataset of Donald Trump's tweets](https://www.kaggle.com/austinreese/trump-tweets), which were collected about 6 weeks ago (Jan 20th, 2020). You should start by downloading the dataset. As usual, please do not commit it to your repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"trumptweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part of the assignment, we'll try to predict the number of retweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "The columns in our dataset are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(a)\n",
    "rubric={points:1}\n",
    "\n",
    "Our target column is `retweets`. Aside from the target column, there is a column here that we cannot use because it would be \"cheating\". What is this column? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(b)\n",
    "rubric={points:1}\n",
    "\n",
    "In addition to the column identified in part (a), there are one or more columns that are useless in our prediction task. What are these column(s)? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(c)\n",
    "rubric={points:10}\n",
    "\n",
    "Parts (a) and (b) aside, we're actually only going to look at the tweets themselves for this assignment, i.e. only the `content` column. Your tasks:\n",
    "\n",
    "- Use `CountVectorizer` to create features from the text data and use `Ridge` as your model. \n",
    "- You will have to find reasonable hyperparameter values. \n",
    "- Use cross-validation on `df_train`. Report your train and cross-validation accuracy after a bit of tuning. \n",
    "- You should log-transform the targets. You can report all your scores as the output of `.score()` for the log-transformed data.\n",
    "- Don't violate the Golden Rule!\n",
    "\n",
    "Benchmark: you should be able to achieve a cross-validation $R^2$ score of at least $0.5$ (on the log-transformed data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(d)\n",
    "rubric={points:2}\n",
    "\n",
    "What are the most important features according to your `Ridge` model from part (c)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(e)\n",
    "rubric={points:4}\n",
    "\n",
    "In part (c) you set up feature extraction from text to vectors using `CountVectorizer`. You may have tweaked some hyperparameters that control the number of features, such as `max_features`. Next, we will apply feature selection using sklearn's `RFECV`. But first - what is the point of applying feature selection here? If we wanted fewer features, why not just change the hyperparameters of `CountVectorizer`, for example by reducing `max_features`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(f)\n",
    "rubric={points:5}\n",
    "\n",
    "Use sklearn's `RFECV` to remove unecessary features from your approach in part (c). You are welcome to use the `step` hyperparameter for speed.\n",
    "\n",
    "1. Did your training error go up or down? Is this what you expected?\n",
    "2. Did your test error go up or down? Is this what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1(g)\n",
    "rubric={points:2}\n",
    "\n",
    "Print out a few of the features that were removed by `RFECV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Let's now try to use pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in class, using pre-trained word embeddings is very common in NLP. These embeddings are created by training a model like [Word2vec](https://en.wikipedia.org/wiki/Word2vec) on a huge corpus of text. In this exercise we will explore a [GloVe](https://nlp.stanford.edu/projects/glove/) model which was trained on a **different** Twitter dataset (not the Trump tweets). Note that the gloVe model is case sensitive and it only has representations of lower-case words.\n",
    "\n",
    "Your tasks are:\n",
    "- Download [GloVe embeddings for Twitter](http://nlp.stanford.edu/data/glove.twitter.27B.zip). This is a large file (the compressed file is ~1.42 GB ). **Please do not commit this file!!** \n",
    "- Unzip the downloaded file. For this exercise we'll be using `glove.twitter.27B/glove.twitter.27B.100d.txt`. The file has words and their corresponding embeddings.\n",
    "- Convert the GloVe embeddings to the Word2vec format using the following command. More details [here](https://www.pydoc.io/pypi/gensim-3.2.0/autoapi/scripts/glove2word2vec/index.html).\n",
    "\n",
    "```\n",
    "python -m gensim.scripts.glove2word2vec -i \"glove.twitter.27B.100d.txt\" -o \"glove.twitter.27B.100d.w2v.txt\"\n",
    "```\n",
    "\n",
    "Then, the following line of could should run (and it may take a few minutes to run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter_model = KeyedVectors.load_word2vec_format('glove.twitter.27B.100d.w2v.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(a)\n",
    "rubric={points:1}\n",
    "\n",
    "Compare the _vocabulary size_ (number of words) of `glove_twitter_model` and your `CountVectorizer` representation from Exercise 1. Which is larger?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove_twitter_model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(b)\n",
    "rubric={points:3}\n",
    "\n",
    "Our pre-trained `glove_twitter_model` gets us a representation of _words_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter_model.get_vector(\"donald\") # equivalent to .transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want a representation of _tweets_, which contain multiple words. The most straightforward way to obtain this is by _averaging_ embeddings of words in the document. Below we are providing a function `get_average_embeddings` which preprocesses the input text and then returns average embedding of all the words in some text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embeddings(text, model = glove_twitter_model):\n",
    "    \"\"\"\n",
    "    Returns the average embedding of the given text\n",
    "    using the given trained model (e.g., Word2vec, FastText). \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    text : (str) input text \n",
    "    model : (gensim.models) model to use to get embeddings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    feat_vect : (numpy.ndarray) the average embedding vector of the given text\n",
    "    \"\"\"\n",
    "    n_features = model.vector_size\n",
    "    \n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    punctuation = string.punctuation\n",
    "    stop_words += list(punctuation)\n",
    "    stop_words.extend(['``','’', '`','br','\"',\"”\", \"''\", \"'s\"])        \n",
    "        \n",
    "    preprocessed = []    \n",
    "    tokenized = word_tokenize(text)\n",
    "    for token in tokenized:\n",
    "        token = token.lower()\n",
    "        if token not in stop_words:\n",
    "            preprocessed.append(token)\n",
    "\n",
    "    feat_vect = np.zeros(n_features, dtype='float64')\n",
    "    #index2word_set = set(model.wv.index2word)    \n",
    "    nwords = 0 \n",
    "\n",
    "    for word in preprocessed:\n",
    "        #if word in index2word_set:\n",
    "        try: \n",
    "            nwords += 1\n",
    "            feat_vect = np.add(feat_vect, model[word])\n",
    "        except:\n",
    "            continue\n",
    "    if nwords == 0:\n",
    "        nwords = 1\n",
    "    feat_vect = np.divide(feat_vect, nwords)     \n",
    "    return feat_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_average_embeddings(\"My name is the Donald\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part (a) you compared the number of words used in each now. Now, compare the _length of the representation_ for this pre-trained model vs. the `CountVectorizer` approach. Briefly discuss. (The pedagogical goal here is for you to distinguish between the number of words and the length of the vector.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(c)\n",
    "rubric={points:5}\n",
    "\n",
    "In Exercise 1 you used `CountVectorizer` to generate features, which were then fed into a regression model. We can do the same here with the features from the pre-trained model (the code may take a few minutes to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_tweets = [get_average_embeddings(review) for review in df_train[\"content\"]]\n",
    "X_test_glove_tweets  = [get_average_embeddings(review) for review in df_test[\"content\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What sort of regression scores can you get with these features instead? Compare with your scores from Exercise 1 and briefly discuss.\n",
    "\n",
    "Benchmark: even after abandoning `Ridge`, I wasn't able to get $R^2$ scores above $0.5$ on the log-transformed targets (although I got close). Maybe you will do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Now we're done with trying to predict the number of retweets. Our next task will be trying to find similar tweets to a query tweet using nearest neighbours, like the product recommendations we discussed in Lecture 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(a) \n",
    "rubric={points:5}\n",
    "\n",
    "Using scikit-learn's `NearestNeighbours` on the word count features from `CountVectorizer`, find the 5 most similar tweets to this made-up tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tweet = \"I am Donald and I am THE BOSS around here #me #sogreat #boss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Euclidean distance and the same `CountVectorizer` hyperparameters you used in Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(b)\n",
    "rubric={points:2}\n",
    "\n",
    "Repeat part (a) but using cosine similarity instead of Euclidean distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(c)\n",
    "rubric={points:5}\n",
    "\n",
    "In lecture we talked about how Euclidean distance resulted in less popular items being recommended than with cosine similarity. What is the analog of \"popularity\" here? Are your results from parts (a) and (b) consistent with this notion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(d)\n",
    "rubric={points:3}\n",
    "\n",
    "Repeat Parts (a) and (b) but this time with the pre-trained gloVe embeddings from Exercise 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(e)\n",
    "rubric={points:1}\n",
    "\n",
    "Did changing the distance metric from Euclidean to cosine have the same sort of effect with the `CountVectorizer` embeddings as with the pre-trained gloVe embeddings? Briefly discuss. \n",
    "\n",
    "Note: this question is quite difficult and I'm not sure I really gave you the necessary tools to answer it. As a compromise, I made it worth only 1 point. If you can't figure it out, I would just move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(f)\n",
    "rubric={points:2}\n",
    "\n",
    "Our first approach, using `CountVectorizer` features, should only retrieve similar tweets if they have some words in common with the query tweet. Is this also true for the pre-trained embedding approach as well? Briefly discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Very short answer questions\n",
    "rubric={points:16}\n",
    "\n",
    "Each question is worth 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After running feature selection with `RFE`, `rfe.ranking_` tells you the order in which the features were removed. Why could this order be different from the order of the original feature importances, ranked from least to most important?\n",
    "2. What are 2 differences between using TFIDF vs. calling `StandardScaler` on the output of `CountVectorizer`?\n",
    "3. True or False (and briefly explain): converting numpy array to a scipy sparse matrix will always reduce its size in memory.\n",
    "4. Why is word tokenization hard - can't you just separate words by searching for the space character (\" \") and splitting on that?\n",
    "5. In Lecture 13 we saw our pre-trained word embedding model output an analogy that reinforced a gender stereotype. Give an example of how using such a model could cause harm in the real world.\n",
    "6. In Lecture 14 we discussed how neural networks are sort of like `Pipeline`s, in the sense that they involve multiple sequential transformations of the data, finally resulting in the prediction. Why was this property useful when it came to transfer learning?\n",
    "7. In Lecture 14 we talked about [this blog post](https://medium.com/@jrzech/what-are-radiological-deep-learning-models-actually-learning-f97a546c5b98). Why is it a problem that the algorithm learned to detect the word \"PORTABLE\"? If that helps get a high accuracy, aren't we happy?\n",
    "8. In Lecture 15 I claimed that supervised KNN is \"interpretable\". If you used `KNeighborsClassifier(n_neighbors=3)` to make a prediction, how would you explain/interpret the prediction to your boss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "_merged",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
