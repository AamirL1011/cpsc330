{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CPSC 330 Lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture plan\n",
    "\n",
    "- ðŸ‘‹\n",
    "- **Turn on recording**\n",
    "- Announcements\n",
    "- True/False questions from last time (10 min)\n",
    "- Pipelines motivation (5 min)\n",
    "- Pipelines (15 min)\n",
    "- Break (5-10 min)\n",
    "- Random forest classifiers (5 min)\n",
    "- Hyperparameter optimization: grid search and random search (20 min)\n",
    "- Bayesian hyperparameter optimization (10 min)\n",
    "- Pipelines and hyperparameter tuning (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Announcements\n",
    "\n",
    "- hw2 solutions posted.\n",
    "- hw3 posted, due Monday at 11:59pm.\n",
    "  - You can work with a partner, see instructions [here](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md#partners)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions from last time (10 min)\n",
    "\n",
    "https://piazza.com/class/kb2e6nwu3uj23?cid=188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines motivation (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our dataset of the week, which is IMDB movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv('data/imdb_master.csv', index_col=0, encoding=\"ISO-8859-1\")\n",
    "imdb_df = imdb_df[imdb_df['label'].str.startswith(('pos','neg'))]\n",
    "imdb_df = imdb_df.sample(frac=0.2, random_state=999) # Take a subsample of the dataset for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train, imdb_test = train_test_split(imdb_df, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, here is what we did last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imdb_raw = imdb_train['review']\n",
    "y_train_imdb = imdb_train['label']\n",
    "\n",
    "X_test_imdb_raw = imdb_test['review']\n",
    "y_test_imdb = imdb_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(min_df=50, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imdb = vec.fit_transform(X_train_imdb_raw)\n",
    "X_test_imdb = vec.transform(X_test_imdb_raw);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_imdb, y_train_imdb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833333333333333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train_imdb, y_train_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_imdb, y_test_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, we avoided cross-validation. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82866667, 0.836     , 0.83733333, 0.83266667, 0.834     ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lr, X_train_imdb, y_train_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code runs.\n",
    "- But we have a problem... our good friend the Golden Rule.\n",
    "- It is actually the exact same problem we fit/transformed the `CountVectorizer` before splitting.\n",
    "- Remember, cross-validation involves splitting!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fold_1 = X_train_imdb[:X_train_imdb.shape[0]//5]\n",
    "X_valid_fold_1 = X_train_imdb[X_train_imdb.shape[0]//5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But wait, the validation part was transformed using a `CountVectorizer` that was fit on the training split.\n",
    "- Just like last time, this is a Golden Rule violation.\n",
    "- For example, the validation split \"gets to be aware of\" words that are only in the training split.\n",
    "\n",
    "So what do we do here?\n",
    "\n",
    "![](img/hmm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter pipelines to the rescue!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "\n",
    "(Pause for Q&A)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- scikit-learn `Pipeline` can help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This time we'll combine **the preprocessing and the model** with a `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(min_df=50, binary=True)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('countvec', countvec),\n",
    "    ('lr', lr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Syntax: pass in a list of **steps**.\n",
    "- The last step should be a model/classifier.\n",
    "- All the earlier steps should be transformers.\n",
    "  - Later in the course we'll see use cases for multiple rounds of transformers, here we only have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train_imdb_raw, y_train_imdb);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is this doing?\n",
    "- Note that I passed in the **raw** text data, not the vectorized word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34838    I guess that \"Gunslinger\" wasn't quite as god-...\n",
       "345      Oh boy! Oh boy! On the cover of worn out VHS h...\n",
       "48840    After you see Vertigo, then watch Bell, Book a...\n",
       "4458     If this film is an accurate display of J. Smit...\n",
       "23815    Only the Brits could make a film like this and...\n",
       "                               ...                        \n",
       "18386    I have probably watched the movie 4 or 5 times...\n",
       "38425    This movie maked me cry at the end! I watch at...\n",
       "46973    This is a weird movie about an archaeologist s...\n",
       "14614    Finally a gangster Movie worth watching!<br />...\n",
       "22094    A few things to touch on as a response to the ...\n",
       "Name: review, Length: 7500, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_imdb_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is doing the following steps:\n",
    "\n",
    "1. Fitting `CountVectorizer`.\n",
    "2. Transforming the data using the fit `CountVectorizer`.\n",
    "3. Fitting the `LinearRegression` on the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `predict` (or `score`), we also feed in the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos', 'pos', 'pos', ..., 'pos', 'pos', 'pos'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X_test_imdb_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a schematic assuming you have two transformers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/pipeline.png\" width=\"700\">\n",
    "\n",
    "[Source](https://amueller.github.io/COMS4995-s20/slides/aml-04-preprocessing/#18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One thing that is awesome here is that we can't make the mistakes we showed last time:\n",
    "  - We call `fit` on the train split and `score` on the test split, it's clean.\n",
    "  - We can't accidentally re-fit the preprocessor on the test data like we did last time.\n",
    "  - It automatically makes sure the same transformations are applied to train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, the moment of truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82666667, 0.824     , 0.83133333, 0.83066667, 0.83533333])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipe, X_train_imdb_raw, y_train_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember what cross-validation does - it calls `fit` and `score`.\n",
    "- Now we're calling `fit` on the pipeline, not just the logistic regression.\n",
    "  - So **both the vectorizer and the logistic regression are refit again on each fold**.\n",
    "  - This is what we want to avoid the Golden Rule violation!\n",
    "  - Every validation score is unseen data with respect to the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/yay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BTW, the scores here aren't that different.\n",
    "- I don't suspect it matters all that much here.\n",
    "- But there could be cases where the effect is large.\n",
    "- In this course I want you to build good habits that will serve you well going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "\n",
    "(Pause for Q&A)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines and the Golden Rule (20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line fits the transformer **and** the model; there was no need to call `preprocessor.fit()` before this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(df_train, y_train_log);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how I passed in `df_train`, not `df_train_enc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.97330353, 11.69417912, 11.9050535 , ..., 12.20638141,\n",
       "       11.99713426, 11.81988184])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8963159928298279"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(df_train, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8913218700485418"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(df_valid, y_valid_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this `Pipeline` on the combined train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91329227, 0.86442022, 0.84424495, 0.87895509, 0.90585488,\n",
       "       0.89379337, 0.86963715, 0.90347339, 0.58863687, 0.91024683])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline, df_trainvalid, y_trainvalid_log, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does this solve the problem?\n",
    "  - Discuss for 2-3 minutes.\n",
    "  - Don't look ahead!\n",
    "  \n",
    "<br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Why does this work?\n",
    "\n",
    "- Because `cross_val_score` calls `fit` for each fold.\n",
    "- And this includes fitting the preprocessor.\n",
    "- Thus, there is actually no difference between `df_train` and `df_valid` - nothing has actually been done to them yet!\n",
    "  - `df_trainvalid` is just the part that's not the test set, that's all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional note) Yet another idea could be to do the cross-validatin using only the validation split. I believe this does not technically violate the Golden Rule, but it's very wasteful in terms of data and is also not really representative of how your model will eventually be trained. The `Pipeline` approach is much better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "\n",
    "(Pause for Q&A)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Break (5-10 min)\n",
    "\n",
    "Please fill out the mid-course survey at https://ubc.ca1.qualtrics.com/jfe/form/SV_6tevNhMjZxRiQEl\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classifiers (5 min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameter optimization: grid search and random search (20 min)\n",
    "\n",
    "#### Manual hyperparameter optimization\n",
    "\n",
    "- We tried this a bit.\n",
    "- Advantage: we may have some intuition about what might work.\n",
    "  - E.g. if I'm massively overfitting, try decreasing `max_depth` or `C`.\n",
    "- Disadvantage: it takes a lot of work.\n",
    "- Disadvantage: in very complicated cases, our intuition might be worse than a data-driven approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Automated hyperparameter optimization\n",
    "\n",
    "- Advantage: reduce human effort\n",
    "- Advantage: less prone to error and improve reproducibility\n",
    "- Advantage: data-driven approaches may be effective\n",
    "- Disadvantage: may be hard to incorporate intuition\n",
    "- Disadvantage: be careful about overfitting on the validation set.\n",
    "\n",
    "\n",
    "\n",
    "There are two automated hyperparameter search methods in scikit-learn:\n",
    "\n",
    "  - Exhaustive grid search: [`sklearn.model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "  - Randomized hyperparameter optimization: [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "  \n",
    "The \"CV\" stands for cross-validation; these searchers have cross-validation built right in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exhaustive grid search\n",
    "\n",
    "- A user specifies a set of values for each hyperparameter. \n",
    "- The method considers \"product\" of the sets and then evaluates each combination one by one.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the automated hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"C\" : [0.01, 1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we can fix some hyperparameters and make others variable.\n",
    "- `verbose=1` tells `GridSearchCV` to print some output while it's working.\n",
    "  - This can be useful as this step sometimes takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    4.9s finished\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train_imdb, y_train_imdb);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to Lecture 3, this is what it's doing:\n",
    "\n",
    "```\n",
    "for C in [0.01, 1, 10, 100]:\n",
    "    for fold in folds:\n",
    "        fit in training portion with the given C\n",
    "        score on validatio portion\n",
    "    compute average score\n",
    "pick hypers with best score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can extract the best hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441333333333333"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the classifier inside like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, max_iter=1000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos', 'pos', 'pos', ..., 'pos', 'pos', 'pos'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.predict(X_test_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also provide some \"syntactic sugar\" and allow you to call `predict` or `score` directly on the `GridSearchCV` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos', 'pos', 'pos', ..., 'pos', 'pos', 'pos'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.predict(X_test_imdb) ## Does the same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ok, so this is all the syntax, but now we know we've been violating the Golden Rule because of the cross-validation.\n",
    "- So let's do it again properly this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [10, 100], 'max_depth': [3, None], 'max_features': [3, None]}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "              \"n_estimators\"     : [10,100],\n",
    "              \"max_depth\"        : [3, None],\n",
    "              \"max_features\"     : [3, None]\n",
    "             }\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How many combinations in total? \n",
    "- $2\\times 2\\times 2=8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(list(map(len, param_grid.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=321)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train_transformed, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'max_features': None, 'n_estimators': 100}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lol... these are the default values.\n",
    "- I guess they picked good defaults!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8549216369281329"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.854922</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>14.519656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.848587</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>1.399292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.848127</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1.986127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.844748</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.517570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.844748</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>5.280527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.839527</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.226066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.761057</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.189320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.760519</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.907236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean_test_score param_max_depth param_max_features  \\\n",
       "rank_test_score                                                       \n",
       "1                       0.854922            None               None   \n",
       "2                       0.848587            None               None   \n",
       "3                       0.848127            None                  3   \n",
       "4                       0.844748               3               None   \n",
       "4                       0.844748               3               None   \n",
       "6                       0.839527            None                  3   \n",
       "7                       0.761057               3                  3   \n",
       "8                       0.760519               3                  3   \n",
       "\n",
       "                param_n_estimators  mean_fit_time  \n",
       "rank_test_score                                    \n",
       "1                              100      14.519656  \n",
       "2                               10       1.399292  \n",
       "3                              100       1.986127  \n",
       "4                               10       0.517570  \n",
       "4                              100       5.280527  \n",
       "6                               10       0.226066  \n",
       "7                               10       0.189320  \n",
       "8                              100       0.907236  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)[['mean_test_score', 'param_max_depth', 'param_max_features', 'param_n_estimators', 'mean_fit_time', 'rank_test_score']].set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that the grid search object acts like a scikit-learn model.\n",
    "- It was actually refit on the _whole_ training set, as discussed earlier in the course!\n",
    "- I believe it is the same as `grid_search.best_estimator_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<=50K', '>50K', '<=50K', ..., '>50K', '<=50K', '<=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problems with exhaustive grid search \n",
    "\n",
    "- Required number of models to evaluate grows exponentially with the dimensionally of the configuration space. \n",
    "- Exhaustive search may become infeasible fairly quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Randomized hyperparameter search\n",
    "\n",
    "- Randomized hyperparameter optimization: [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "- Samples configurations at random until certain budget (e.g., time) is exhausted.\n",
    "- Advantage: you can choose how many runs you'll do.\n",
    "- Advantage: you can restrict yourself less on what values you might try.\n",
    "- Advantage: Adding parameters that do not influence the performance does not affect efficiency.\n",
    "- Advantage: research shows this is generally a better idea than grid search, see image for intuition:\n",
    "\n",
    "![](img/randomsearch_bergstra.png)\n",
    "\n",
    "Source: [Bergstra and Bengio, Random Search for Hyper-Parameter Optimization, JMLR 2012](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).\n",
    "\n",
    "- You don't know in advance which hyperparameters are important for your problem.\n",
    "- But some of them might be unimportant.\n",
    "- In the left figure, 6 of the 9 searches are useless because they are only varying the unimportant parameter.\n",
    "- In the right figure, all 9 searches are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to syntax. We can have the parameters chosen from a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_choices = {\n",
    "              \"n_estimators\"     : [10, 30, 100, 300],\n",
    "              \"max_depth\"        : [3, 10, None],\n",
    "              \"max_features\"     : [3, 10, None]\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also give it distributions, instead of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "              \"n_estimators\"     : scipy.stats.randint(low=10, high=300),\n",
    "              \"max_depth\"        : scipy.stats.randint(low=10, high=30),\n",
    "              \"max_features\"     : scipy.stats.randint(low=10, high=30)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=321) # Note: you can set other hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(rf, param_distributions = param_dist, \n",
    "                                   n_iter = 10, \n",
    "                                   cv=3,\n",
    "                                   verbose=1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "random_search.fit(X_train_transformed, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: some hyperparameters significantly affect the training time!\n",
    "- For example, setting `n_estimators=1000` is going to be very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 16, 'max_features': 27, 'n_estimators': 93}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we get something different! \n",
    "- What's the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.863175750441226"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, we had 85.4% and now we have 86.1%.\n",
    "- Is that difference important?\n",
    "- Do we BELIEVE that difference?\n",
    "  - We can try it out on the test set.\n",
    "- But first:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.863176</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>93</td>\n",
       "      <td>4.662635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.862561</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>106</td>\n",
       "      <td>2.776636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.861640</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>108</td>\n",
       "      <td>3.506636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.861602</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>94</td>\n",
       "      <td>2.629149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.861333</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>218</td>\n",
       "      <td>5.281985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.860527</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>83</td>\n",
       "      <td>3.811981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.860450</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>263</td>\n",
       "      <td>15.884677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.860412</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>234</td>\n",
       "      <td>8.109985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.859951</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>145</td>\n",
       "      <td>7.407122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.859375</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>0.542452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mean_test_score param_max_depth param_max_features  \\\n",
       "rank_test_score                                                       \n",
       "1                       0.863176              16                 27   \n",
       "2                       0.862561              20                 11   \n",
       "3                       0.861640              23                 12   \n",
       "4                       0.861602              17                 12   \n",
       "5                       0.861333              14                 10   \n",
       "6                       0.860527              27                 25   \n",
       "7                       0.860450              25                 29   \n",
       "8                       0.860412              10                 24   \n",
       "9                       0.859951              25                 26   \n",
       "10                      0.859375              14                 27   \n",
       "\n",
       "                param_n_estimators  mean_fit_time  \n",
       "rank_test_score                                    \n",
       "1                               93       4.662635  \n",
       "2                              106       2.776636  \n",
       "3                              108       3.506636  \n",
       "4                               94       2.629149  \n",
       "5                              218       5.281985  \n",
       "6                               83       3.811981  \n",
       "7                              263      15.884677  \n",
       "8                              234       8.109985  \n",
       "9                              145       7.407122  \n",
       "10                              12       0.542452  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(random_search.cv_results_)[['mean_test_score', 'param_max_depth', 'param_max_features', 'param_n_estimators', 'mean_fit_time', 'rank_test_score']].set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Look at the timings, they are quite interesting.\n",
    "- And now, the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8527560264087211"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8622754491017964"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "\n",
    "(Pause for Q&A)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian hyperparameter optimization (10 min)\n",
    "\n",
    "- Both `GridSearchCV` and `RandomizedSearchCV` do each trial independently.\n",
    "- What if you could learn from your experience, e.g. learn that `max_depth=3` is bad?\n",
    "  - That could save time because you wouldn't try combinations involving `max_depth=3` in the future.\n",
    "- We can do this with `scikit-optimize`, which is a completely different package from `scikit-learn`\n",
    "- It uses a technique called \"model-based optimization\" and we'll specifically use \"Bayesian optimization\".\n",
    "  - In short, it uses machine learning to predict what hyperparameters will be good.\n",
    "  - Machine learning on machine learning!\n",
    "- As it happens I did my PhD thesis on this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `BayesSearchCV` uses the same interface as `GridSearchCV` and `RandomSearchCV`.\n",
    "- However, the way we specify the parameter distributions is slightly different.\n",
    "- Here, we can just give the bounds as tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_opt = BayesSearchCV(\n",
    "    RandomForestClassifier(random_state=321),\n",
    "    {\n",
    "        'n_estimators': (10, 300),  \n",
    "        'max_depth': (3, 30),\n",
    "        'max_features': (3, 30)\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    random_state=123,\n",
    "    verbose=0,\n",
    "    refit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 41s, sys: 3.35 s, total: 2min 44s\n",
      "Wall time: 3min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=3, error_score='raise',\n",
       "              estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                               class_weight=None,\n",
       "                                               criterion='gini', max_depth=None,\n",
       "                                               max_features='auto',\n",
       "                                               max_leaf_nodes=None,\n",
       "                                               max_samples=None,\n",
       "                                               min_impurity_decrease=0.0,\n",
       "                                               min_impurity_split=None,\n",
       "                                               min_samples_leaf=1,\n",
       "                                               min_samples_split=2,\n",
       "                                               min_weight_fraction_leaf=0.0,\n",
       "                                               n_estimators=100, n_jobs=None,\n",
       "                                               oob_score=False,\n",
       "                                               random_state=321, verbose=0,\n",
       "                                               warm_start=False),\n",
       "              fit_params=None, iid=True, n_iter=10, n_jobs=1, n_points=1,\n",
       "              optimizer_kwargs=None, pre_dispatch='2*n_jobs', random_state=123,\n",
       "              refit=True, return_train_score=False, scoring=None,\n",
       "              search_spaces={'max_depth': (3, 30), 'max_features': (3, 30),\n",
       "                             'n_estimators': (10, 300)},\n",
       "              verbose=0)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bayes_opt.fit(X_train_transformed, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It took a similar amount of time to the other methods.\n",
    "- In reality there is some extra computation to do the \"meta-ML\".\n",
    "- However, the overall time is dominated by the time of calling `fit` on the random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 18, 'max_features': 12, 'n_estimators': 130}"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8625998157248157"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_opt.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The score looks promising.\n",
    "- In theory, it should get even better as we increase `n_iter` (because it has more data to learn from).\n",
    "- Checking the test score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8621219100261016"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_opt.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And reproducing the previous test scores for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8622754491017964"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8527560264087211"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case, it seems we weren't overfitting on the validation set and this exercise was actually useful.\n",
    "- Should I always use this? Not necessarily.\n",
    "- Disadvantage: requires installation.\n",
    "- Disadvantage: when number of trials is large (e.g. hundreds), the meta-ML can actually get too slow.\n",
    "- Disadvantage: harder parallelize the search because each trial depends on the previous ones.\n",
    "  - Note `n_jobs` parameter for `GridSearchCV` and `RandomizedSearchCV`.  \n",
    "  - `BayesSearchCV` also has this parameter.\n",
    "  - It can definitely parallelize the folds.\n",
    "  - The search will be less effective if it parallelizes further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can I generalize this to say `BayesSearchCV` > `RandomizedSearchCV` > `GridSearchCV`?\n",
    "- Not quite. I'd say `RandomizedSearchCV` > `GridSearchCV` is pretty reasonable\n",
    "- But we should think a bit more carefully about `BayesSearchCV` for the above reasons.\n",
    "- `RandomizedSearchCV` is often a reasonable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines and hyperparameter optimization (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The same problems arise when doing hyperparameter optimization, simply because these methods do cross-validation.\n",
    "- We can avoid them with a `Pipeline` in the same way.\n",
    "- I'll optimize `alpha`, so I'll create a new pipeline where `alpha` is not specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('preprocessor', preprocessor),\n",
    "                      ('model', Ridge())])\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__numeric__imputer__strategy': ['mean', 'median'],\n",
    "    'model__alpha': [1.0, 10, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above: we have a nesting of transformers. \n",
    "- We can access the parameters of the \"inner\" objects by using `__` to go \"deeper\":\n",
    "  - `model__alpha`: \"the `alpha` of the model (of the pipeline)\"\n",
    "  - `preprocessor__numeric__imputer__strategy`: \"the strategy of the imputer of the numeric transformer of the preprocessor (of the pipeline)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, param_grid, cv=10)\n",
    "grid_search.fit(df_trainvalid, y_trainvalid_log);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 10, 'preprocessor__numeric__imputer__strategy': 'median'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particularly useful when there are serious hyperparameters in the preprocessing pipeline, e.g. if you're using `CountVectorizer`."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
