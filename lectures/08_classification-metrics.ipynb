{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CPSC 330 Lecture 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture plan\n",
    "\n",
    "- Precision, recall, confusion matrices (15 min)\n",
    "- Evaluation metrics True/False (10 min)\n",
    "- ROC (20 min)\n",
    "- Break (5 min)\n",
    "- Addressing class imbalance (10 min)\n",
    "- Stratified splits (5 min)\n",
    "- This week - summary (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "more strongly discourage the over-reliance on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Should plot TPR and FPR on y-axis vs. threshold on x-axis, I think this would make the ROC curve a lot more clear. \n",
    "- Should talk more about the user setting the threshold to achieve a desired TPR or FPR, instead of using 'class_weight`. \n",
    "- Throughout the course should may more attention to metrics like AUC rather than accuracy.\n",
    "- See [here](https://amueller.github.io/COMS4995-s20/slides/aml-09-model-evaluation/#20) for nicer code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from piazza\n",
    "\n",
    "Can we interpret this as: When we lower our threshold of predict_proba, we have a higher recall (because we are predicting more positives) but we also have a lower precision (because we are increasing the number of FN's)?\n",
    "\n",
    "The only that that is guaranteed is that when you lower the threshold you will predict as positive all those instances you were previously predicting as positive, plus possibly more. Recall is TP/positives. The number of positives is unaffected by your threshold, so if TP goes up then recall goes up. Thus, when you decrease the threshold, your recall might stay the same or it might go up. However, the precision is another story - it could actually go down. Precision is defined as TP/(TP+FP). If your model produced really bad histograms, it might be the case that lowering the threshold would cause you to predict more positives, but the true class for all those cases was negative. Thus TP would stay the same and FP would go up, so precision would go down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import tree \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import normalize, scale, Normalizer, StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra dependency for the next import:\n",
    "\n",
    "```\n",
    "pip install scikit-optimize\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following import requires scikit-learn version >= 0.22.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def show_scores(model, X_train, y_train, X_valid, y_valid):\n",
    "    print(\"Training error: %.4f\" % ((1-model.score(X_train, y_train))))\n",
    "    print(\"Test     error: %.4f\" % ((1-model.score(X_valid, y_valid))))\n",
    "def show_scores_percent(model, X_train, y_train, X_valid, y_valid):\n",
    "    print(\"Training error: %.3f%%\" % (100*(1-model.score(X_train, y_train))))\n",
    "    print(\"Test     error: %.3f%%\" % (100*(1-model.score(X_valid, y_valid))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision, recall, confusion matrices (15 min)\n",
    "\n",
    "- Let's classify fraudulent and non-fraudulent transactions using Kaggle's [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) data set.\n",
    "- For confidentially reasons, it only provides transformed features with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cc_df = pd.read_csv('data/creditcard.csv', encoding='latin-1')\n",
    "cc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = cc_df.drop(columns = ['Class'])\n",
    "y = cc_df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At this point we would want to thinm more carefully about the Time and Amount variables.\n",
    "- We could scale Amount, but do we want to scale Time?\n",
    "- More on time series coming later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=500)\n",
    "lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores(lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svm = SVC(max_iter=500, gamma='scale')\n",
    "svm.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores(svm, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We have an excellent accuracy!! \n",
    "- But how reliable is it?\n",
    "- Would this model be able to identify fraudulent transactions? \n",
    "- Let's examine to what extent the model is able to identify fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### What's the class distribution\n",
    "cc_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `confusion_matrix` instead of `plot_confusion_matrix` if your version of scikit-learn is too old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "disp = plot_confusion_matrix(lr, X_test, y_test,\n",
    "                             display_labels=['Non fraud', 'Fraud'],\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             values_format = 'd')\n",
    "disp.ax_.set_title('Confusion matrix for logistic regression');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(disp.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "disp_svm = plot_confusion_matrix(svm, X_test, y_test,\n",
    "                             display_labels=['Non fraud', 'Fraud'],\n",
    "                             cmap=plt.cm.Reds, \n",
    "                             values_format = 'd')\n",
    "disp_svm.ax_.set_title('Confusion matrix for SVM');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(disp_svm.confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution**: \n",
    "\n",
    "- scikit-learn's convention is to have true label as the rows, predicted as the columns. \n",
    "- Others do it the other way around, e.g. the [confusion matrix Wikipedia article](https://en.wikipedia.org/wiki/Confusion_matrix). \n",
    "- You can always transpose the confusion matrix with `.T`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(disp_svm.confusion_matrix.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there doesn't seem to be a way with `plot_confusion_matrix`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Let's understand confusion matrix\n",
    "\n",
    "Using the sklearn convention:\n",
    "\n",
    "|   X   | predict negative | predict positive |\n",
    "|------|----------|-------|\n",
    "| negative example | True negative (TN) | False positive (FP)|\n",
    "| positive example | False negative (FN) | True positive (TP) |\n",
    "\n",
    "- Perfect prediction has all values down the diagonal\n",
    "- Off diagonal entries can often tell us about what is being mis-predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Accuracy does not tell the whole story!\n",
    "\n",
    "- Accuracy is misleading when we have class imbalance. \n",
    "- We need other metrics to evaluate our models. \n",
    "- We'll first discuss model evaluation.\n",
    "- And then what can be done about class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics cheat sheet:\n",
    "\n",
    "<img src='img/evaluation-metrics.png' width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Precision and recall\n",
    "\n",
    "Recall: how many of the actual positive examples did you identify?\n",
    "\n",
    "Precision: of the positive examples you identify, how many were real?\n",
    "\n",
    "- Consider a disease diagnosis system. Suppose you have 100 examples: 20 with disease = 'yes' and 80 with disease = 'no'. \n",
    "- High precision system example: Consider a very conservative system that classifies examples as 'yes' only in the presence of certain value for a strong feature. Suppose in your dataset there is only one such example. \n",
    "    - Precision = TP/(TP + FP) = 1 / (1 + 0) = 1\n",
    "    - What's the recall in this case? \n",
    "- High recall system example: Consider a system which generously classifies all examples as 'yes'. \n",
    "    - Recall = TP/(TP + FN) = 20 / (20 + 0) = 1\n",
    "    - What's the precision in this case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: you may have heard other terms like sensitivity and specificity. \n",
    "- See the [MDS terminology document](https://ubc-mds.github.io/resources_pages/terminology/) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Type I, Type II errors\n",
    "\n",
    "- Type I error is another name for false positive\n",
    "- Type II error is another name for false negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predictions = lr.predict(X_test)\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, predictions).ravel()\n",
    "print('Confusion matrix for fraud data set')\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#### Calculate evaluation metrics \n",
    "\n",
    "accuracy = (TP+TN)/(TN+FP+FN+TP)\n",
    "error = (FP+FN)/(TN+FP+FN+TP)\n",
    "precision = TP/(TP + FP)\n",
    "recall = TP/(TP + FN)\n",
    "f1 = 2* (precision * recall)/(precision + recall)\n",
    "data = {}\n",
    "data['accuracy'] = [accuracy]\n",
    "data['error'] = [error]\n",
    "data['precision'] = [precision]\n",
    "data['recall'] = [recall]\n",
    "data['f1'] = [f1]\n",
    "\n",
    "metrics_df = pd.DataFrame(data)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is for logistic regression. We can do the same for the SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, svm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it can be useful to normalize the confusion matrix so that it shows proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_confusion_matrix(lr, X_test, y_test,\n",
    "                             display_labels=['Non fraud', 'Fraud'],\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             values_format = 'g',\n",
    "                             normalize='all')\n",
    "disp.ax_.set_title('Confusion matrix for logistic regression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can normalize in various ways, and thus read precision/recall right off the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_confusion_matrix(lr, X_test, y_test,\n",
    "                             display_labels=['Non fraud', 'Fraud'],\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             values_format = 'g',\n",
    "                             normalize='true')\n",
    "disp.ax_.set_title('Confusion matrix for logistic regression');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_confusion_matrix(lr, X_test, y_test,\n",
    "                             display_labels=['Non fraud', 'Fraud'],\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             values_format = 'g',\n",
    "                             normalize='pred')\n",
    "disp.ax_.set_title('Confusion matrix for logistic regression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Confusion matrix for multi-class: \n",
    "\n",
    "- We will talk about multi-class classification later.\n",
    "- But for now, FYI we can still compute a confusion matrix, e.g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation metrics True/False (15 min)\n",
    "\n",
    "1. In medical diagnosis, false positives are more damaging than false negatives (assume \"positive\" means the person has a disease, \"negative\" means they don't).\n",
    "2. In spam classification, false positives are more damaging than false negatives (assume \"positive\" means the email is spam, \"negative\" means they it's not).\n",
    "3. In the medical diagnosis, high recall is more important than high precision.\n",
    "4. If method A gets a higher accuracy than method B, that means its precision is also higher.\n",
    "5. If method A gets a higher accuracy than method B, that means its recall is also higher.\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Receiver Operating Characteristic (ROC) Curve (10 min)\n",
    "\n",
    "- What if you care about certain types of mistakes more than others?\n",
    "- In the medical diagnosis example, you want to be conservative and bias the model towards predicting \"positive\".\n",
    "- We said earlier that `predict` returns 1 when `predict_proba`'s probabilities is above 0.5.\n",
    "- Key idea: what if we threshold the probability at a different values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, lr.predict_proba(X_test)[:,1]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, lr.predict_proba(X_test)[:,1]>0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, lr.predict_proba(X_test)[:,1]>0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we lower the threshold for classifying something as fraudulent, we end up predicting more fraud. \n",
    "- (Strangely, doing so seems to improve overall accuracy in this case! That shouldn't be true in general...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve\n",
    "\n",
    "- [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) looks at what happens as you change this threshold.\n",
    "- ROC plots true positive rate (recall) ($\\frac{TP}{TP + FN}$) against false positive rate ($\\frac{FP}{\\# negatives }$) \n",
    "- Note that the \"C\" doesn't stand for \"curve\", so saying \"ROC curve\" isn't like saying \"PIN number\" or \"SIN number\".\n",
    "- The diagonal line is what happens if your guesses are random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1])\n",
    "\n",
    "plt.plot(fpr, tpr);\n",
    "plt.plot((0,1),(0,1),'--k');\n",
    "ind = np.argmin(np.abs(thresholds-0.5))\n",
    "plt.plot(fpr[ind], tpr[ind], 'ro', markersize=13)\n",
    "plt.xlabel('false positive rate');\n",
    "plt.ylabel('true positive rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above: the red circle corresponds to the threshold of 0.5, which is used by `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the threshold is 1, we always predict \"negative\".\n",
    "  - the true positive rate is 0 because there are no true positives\n",
    "  - the false positive rate is 0 because there are no false positives\n",
    "  - this is the bottom-left point in the curve\n",
    "- If the threshold is 0, we always predict \"positive\".\n",
    "  - the true positive rate (recall) is 1 because we have identified all positive examples\n",
    "  - the false positive rate is 1 because we have falsely identified all negatives as positive\n",
    "  - this is the upper-right point in the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make random predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_preds = np.random.rand(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, random_preds)\n",
    "\n",
    "plt.plot(fpr, tpr);\n",
    "plt.plot((0,1),(0,1),'--k');\n",
    "plt.xlabel('false positive rate');\n",
    "plt.ylabel('true positive rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular metric is the area under the curve (AUC):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, lr.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, random_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dariyasydykova/open_projects/master/ROC_animation/animations/cutoff.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Area under the curve (AUC)\n",
    "\n",
    "- AUC provides a single meaningful number for systems performance \n",
    "- AUC of 1.0 means perfect classification and AUC of 0.5 means random chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/dariyasydykova/open_projects/master/ROC_animation/animations/ROC.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing class imbalance (10 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Unbalanced training sets\n",
    "\n",
    "- This typically refers to having many more examples of one class than another in one's training set.\n",
    "- Our Credit Card Fraud dataset is unbalanced.\n",
    "- A lot of problems are coming from this class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do I want to address the problem?\n",
    "\n",
    "- An important question is whether the class imbalance is a problem.\n",
    "- Ask yourself, do I care about type I errors more than type II errors?\n",
    "- If not, then it's fine to have a confusion matrix like we have for logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr, X_test, y_test,\n",
    "                             display_labels=['Non fraud', 'Fraud'],\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             values_format = 'd');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I'd say it's probably not fine to have a confusion matrix like the one for SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(svm, X_test, y_test,\n",
    "                             display_labels=['Non fraud', 'Fraud'],\n",
    "                             cmap=plt.cm.Reds, \n",
    "                             values_format = 'd');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because you probably don't want a model that never predicts \"fraud\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many sklearn classifiers have a parameter called `class_weight`.\n",
    "- This allows you to specify that one class is more important than another.\n",
    "- For example, maybe a false negative is 10x more problematic than a false positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_weight = LogisticRegression(max_iter=500, class_weight={1:10}) # set fraud a weight of 10\n",
    "lr_weight.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr_weight, X_test, y_test, display_labels=['Non fraud', 'Fraud'], cmap=plt.cm.Blues, values_format = 'd');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice we've predicted more Fraud this time.\n",
    "- This was equivalent to saying \"repeat every positive example 10x in the training set\".\n",
    "  - But repeating data would slow down the code, whereas this doesn't.\n",
    "- A useful setting is `class_weight=\"balanced\"`\n",
    "- This sets the weights so that the classes are \"equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_weight_balanced = LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    "lr_weight_balanced.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr_weight_balanced, X_test, y_test, display_labels=['Non fraud', 'Fraud'], cmap=plt.cm.Blues, values_format = 'd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1])\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_test, lr_weight_balanced.predict_proba(X_test)[:,1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"original\");\n",
    "plt.plot(fpr2, tpr2, label=\"balanced\");\n",
    "\n",
    "plt.plot((0,1),(0,1),'--k');\n",
    "plt.xlabel('false positive rate');\n",
    "plt.ylabel('true positive rate');\n",
    "plt.title(\"ROC curve\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above: the balanced version has a better ROC curve, even though it has worse accuracy (see below).\n",
    "- We can make the plot again but this time showing the threshold=0.5 point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1])\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_test, lr_weight_balanced.predict_proba(X_test)[:,1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"original\");\n",
    "plt.plot(fpr2, tpr2, label=\"balanced\");\n",
    "\n",
    "ind = np.argmin(np.abs(thresholds-0.5))\n",
    "plt.plot(fpr[ind], tpr[ind], 'bo', markersize=13)\n",
    "\n",
    "ind2 = np.argmin(np.abs(thresholds2-0.5))\n",
    "plt.plot(fpr2[ind2], tpr2[ind2], 'yo', markersize=13)\n",
    "\n",
    "plt.plot((0,1),(0,1),'--k');\n",
    "plt.xlabel('false positive rate');\n",
    "plt.ylabel('true positive rate');\n",
    "plt.title(\"ROC curve\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: changing the class weight will **generally reduce accuracy**.\n",
    "\n",
    "- The original model was trying to maximize accuracy.\n",
    "- Now you're telling it to do something different.\n",
    "- But that can be fine, accuracy isn't the only metric that matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores(lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores(lr_weight, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores(lr_weight_balanced, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So what's going on here?\n",
    "- Because there are so many more negative examples than positive, false positives affect accuracy much more than false negatives.\n",
    "- Thus, precision matters a lot more than recall.\n",
    "- So, the default method trades off a lot of recall for a bit of precision.\n",
    "- This maximizes accuracy overall - but do we like it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) BTW, what is \"balanced\" doing exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have 587 times more negatives examples, so let's repeat our positive examples so that we have 587x more of them.\n",
    "- Then the classes are \"balanced\".\n",
    "- We can reproduce the result by explicitly setting the class weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_weight_balanced_2 = LogisticRegression(max_iter=500, class_weight={1:ratio})\n",
    "lr_weight_balanced_2.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr_weight_balanced_2, X_test, y_test, display_labels=['Non fraud', 'Fraud'], cmap=plt.cm.Blues, values_format = 'd');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A very important question to ask yourself: \"Why do I have a class imbalance?\"\n",
    "  - Is it because one class is much more rare than the other?\n",
    "  - Is it because of my data collection methods?\n",
    "- If it's the data collection, then that means _your deployment and training data come from different distributions_!\n",
    "  - It's reasonable to use `class_weight` to try and fix this.\n",
    "  - If it's just because one is more rare than the other, you need to ask whether you care about one type of error more than the other.\n",
    "  - If so, it's again reasonable to use `class_weight` to address this.\n",
    "  \n",
    "But, if you answer \"no\" to both question, it may be fine to just ignore the class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note about the predicted probabilities\n",
    "\n",
    "- Note that if you set `class_weight` because you care about one type of error more than the other, the output of `predict_proba` loses some of its meaning.\n",
    "- It's not really true that you have a 80% chance of fraud for this example. \n",
    "- It's really 20%, but you don't want false negatives.\n",
    "- What we're really doing is changing the threshold, but it's common to do it this way.\n",
    "- On the other hand, if you are using `class_weight` because it was harder to collect data for one of the classes, that is all fine.\n",
    "- Be careful about how you interpret the probabilities in general, and extra careful when using `class_weight`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to our dataset of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/adult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=['income'])\n",
    "y_train = df_train['income']\n",
    "X_test  = df_test.drop(columns=['income'])\n",
    "y_test  = df_test['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features     = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
    "categorical_features = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers=[\n",
    "    ('scale', StandardScaler(), numeric_features),\n",
    "    ('ohe', OneHotEncoder(drop='first', sparse=False), categorical_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = numeric_features + list(preprocessor.named_transformers_['ohe'].get_feature_names(categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = pd.DataFrame(preprocessor.transform(X_train), index=X_train.index, columns=new_columns)\n",
    "X_test_transformed  = pd.DataFrame(preprocessor.transform(X_test),  index=X_test.index,  columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: do we have a class imbalance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['income'].value_counts()/len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this class imbalance a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=500)\n",
    "lr.fit(X_train_transformed, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr, X_test_transformed, y_test, display_labels=df_train['income'].unique(), cmap=plt.cm.Blues, values_format = 'g', normalize=\"true\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can we use this information to improve our model?\n",
    "- Well, why are we doing this in the first place?\n",
    "- Are low/high income people more likely to fill out the census?\n",
    "- Etc.\n",
    "\n",
    "_These questions are just as important as choosing the \"best\" model, if not more important!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Splits (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A similar idea of \"balancing\" classes can be applied to data splits.\n",
    "- For example, with cross-validation, there is also [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html).\n",
    "- From the documentation:\n",
    "\n",
    "> This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n",
    "\n",
    "- In other words, if we have 10% negative examples in total, then each fold will have 10% negative examples.\n",
    "- We have the same option in `train_test_split` with the `stratify` argument. \n",
    "- Is this a good idea? \n",
    "  - Well, it's no longer a random sample, which is probably theoretically bad, but not that big of a deal.\n",
    "  - If you have many examples, it shouldn't matter as much.\n",
    "  - It can be especially useful in multi-class, say if you have one class with very few cases.\n",
    "  - In general, these are difficult questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This week - Summary (5 min)\n",
    "\n",
    "- This week's dataset: predicting income level from adult census data.\n",
    "  - Problem: I am too lazy to explore my dataset\n",
    "  - Solution: `pandas_profiler`\n",
    "  - Problem: numerical features: big values drown out other features.\n",
    "  - Solution: scaling.\n",
    "  - Problem: categorical features.\n",
    "  - Solution: one-hot encoding and ordinal encoding (need domain knowledge to choose which one).\n",
    "  - Problem: missing data.\n",
    "  - Solution: impute something simple (legitimate concerns raised - we may come back to this in more depth)\n",
    "  - Problem: how do I choose a model?\n",
    "  - Solution: it's hard - but think about model complexity, speed, interpretability.\n",
    "  - Problem: \"I'm not sure if 15% accuracy is good?\"\n",
    "  - Solution: start with a baseline.\n",
    "  - Problem: people spending too much time on fancy models.\n",
    "  - Solution: start with a baseline!\n",
    "  - Problem: \"surely 99.9% accuracy is good?!\"\n",
    "  - Solution: look at metrics beyond accuracy (e.g. confusion matrix)\n",
    "  - Problem: \"I care more about false negatives than false positives\"\n",
    "  - Solution: change probability threshold or use `class_weight`\n",
    "  - Problem: \"I have a lot more negative examples in my dataset\"\n",
    "  - Solution: ask yourself, is that really a problem? If so, use `class_weight`\n",
    "  - Problem: Hyperparameter optimization is a huge pain.\n",
    "  - Solution: it is still a huge pain, but automated methods help a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next week - current plan\n",
    "\n",
    "- Regression: target value is continuous, not categorical\n",
    "- I am planning on a real estate dataset: predicting house prices.\n",
    "  - Let's see if I can find a good one.\n",
    "- Some new and interesting problems related to regression.\n",
    "  - E.g., should I scale my target values?\n",
    "- Some new and interesting general problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
