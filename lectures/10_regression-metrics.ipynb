{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CPSC 330 Lecture 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture plan\n",
    "\n",
    "- ðŸ‘‹\n",
    "- **Turn on recording**\n",
    "- Announcements (5 min)\n",
    "- Dataset of the week: regression! (20 min)\n",
    "- Linear regression intro (5 min)\n",
    "- Regression score functions: mean squared error and R^2 (10 min)\n",
    "- Break (5 min)\n",
    "- Exploring `alpha` (5 min)\n",
    "- Transforming the targets (20 min)\n",
    "- Ensembling with regression (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Announcements\n",
    "\n",
    "Homework assignments:\n",
    "\n",
    "- hw4 deadline passed\n",
    "- hw5 coming today, due Monday at 11:59pm\n",
    "- After that a 2-week break from hw \n",
    "\n",
    "Midterm:\n",
    "\n",
    "- a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- make some feature; why not price per square foot? (because it uses the target!)\n",
    "- Also: MoSold (month sold) feature - to discuss?\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set of the week (20 min)\n",
    "\n",
    "This week in lecture we will be focussing on the [Kaggle House Prices dataset](https://www.kaggle.com/c/home-data-for-ml-course/). As usual, to run this notebook you'll need to download the data. Unzip the data. Rename `train.csv` to `housing.csv` and move it into the data directory of this repo. (For this dataset, train and test have already been separated. The \"test\" data they provide is actually what we call deployment. They only provide the X, not the y. So we couldn't actually use Kaggle's test set as our test set, since we need the y values for that.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, the target is `SalePrice`. Note that this is numeric, not categorical.\n",
    "- In this case, we call the task **regression** (as opposed to classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: `pandas_profiler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df_train, title='Pandas Profiling Report') #, minimal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From here we can see a mix of feature types, and a bunch of missing values. \n",
    "- Now, I do some of the dirty work...\n",
    "- There are a bunch of ordinal features using the same scale: excellent, good, average, etc.\n",
    "  - These I'm calling `ordinal_features_reg`.\n",
    "- There are a bunch more ordinal features using different scales.\n",
    "  - These I'm calling `ordinal_features_oth`\n",
    "  - To save time I'm ignoring the ordinality and just encoding them as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target               = ['SalePrice']\n",
    "drop_features        = ['Id']\n",
    "numeric_features     = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', \n",
    "                        'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n",
    "                        'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', \n",
    "                        'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', \n",
    "                        'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', \n",
    "                        'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', \n",
    "                        'ScreenPorch', 'PoolArea', 'MiscVal', 'YrSold']\n",
    "ordinal_features_reg = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', \n",
    "                        'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "ordinal_features_oth = ['BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n",
    "                        'Functional',  'Fence']\n",
    "categorical_features = list(set(df_train.columns) - set(target) - set(drop_features) - \n",
    "                            set(numeric_features) - \n",
    "                            set(ordinal_features_reg) - set(ordinal_features_oth))\n",
    "all_features = numeric_features + ordinal_features_reg + categorical_features + ordinal_features_oth\n",
    "\n",
    "ordering = ['Po', 'Fa', 'TA', 'Gd', 'Ex'] # if N/A it will just impute something, per below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: data quality\n",
    "\n",
    "- We'll just use `SimpleImputer` again.\n",
    "- In reality we'd want to go through this more carefully.\n",
    "- We may also want to drop some columns that are almost entirely missing.\n",
    "- We could also check for outliers, and do other EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers = [\n",
    "    ('numeric', SimpleImputer(strategy='median'), numeric_features),\n",
    "    ('ordinal', SimpleImputer(strategy='most_frequent'), ordinal_features_reg),\n",
    "    ('categor', SimpleImputer(strategy='constant', fill_value=\"?\"), \n",
    "     categorical_features + ordinal_features_oth)]\n",
    "# NOTE: the order here must match the order of all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_transformer = ColumnTransformer(transformers=imputers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_transformer.fit(df_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_imp = pd.DataFrame(impute_transformer.transform(df_train), index=df_train.index, columns=all_features)\n",
    "df_valid_imp = pd.DataFrame(impute_transformer.transform(df_valid), index=df_valid.index, columns=all_features)\n",
    "df_test_imp  = pd.DataFrame(impute_transformer.transform(df_test),  index=df_test.index,  columns=all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_imp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: feature transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transformers = [\n",
    "    ('scale',  StandardScaler(), numeric_features),\n",
    "    ('ord',    OrdinalEncoder(categories=[ordering for i in ordinal_features_reg]), ordinal_features_reg),\n",
    "    ('ohe',    OneHotEncoder(drop='first', sparse=False), categorical_features + ordinal_features_oth) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_preprocessor = ColumnTransformer(transformers=feature_transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_preprocessor.fit(df_train_imp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = numeric_features + ordinal_features_reg + list(feature_preprocessor.named_transformers_['ohe'].get_feature_names(categorical_features + ordinal_features_oth))\n",
    "new_columns;\n",
    "# NOTE: the order here must match the order above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "df_train_imp_encode = pd.DataFrame(feature_preprocessor.transform(df_train_imp), index=df_train_imp.index, columns=new_columns)\n",
    "df_valid_imp_encode = pd.DataFrame(feature_preprocessor.transform(df_valid_imp), index=df_valid_imp.index, columns=new_columns)\n",
    "df_test_imp_encode  = pd.DataFrame(feature_preprocessor.transform(df_test_imp),  index=df_test_imp.index,  columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened here?\n",
    "\n",
    "- We fit the `OneHotEncoder` on the training data.\n",
    "- Due to the splits, none of the training examples had these categories, but the validation/test set has them.\n",
    "- How do we want to handle this? \n",
    "- It depends!\n",
    "- Option 1: fit the OHE on _all_ data.\n",
    "  - But isn't this violating the Golden Rule??\n",
    "  - Well, do we know the categories in advance?\n",
    "  - Or is there a chance we'll see a new category in deployment? \n",
    "- Option 2: have a special category for \"unknown\".\n",
    "  - We can reserve all zeros, but then we can't drop the first.\n",
    "  - Let's do Option 2, to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transformers = [\n",
    "    ('scale',  StandardScaler(), numeric_features),\n",
    "    ('ord',    OrdinalEncoder(categories=[ordering for i in ordinal_features_reg]), ordinal_features_reg),\n",
    "    # note changes           VVVVVVVVVV       and     VVVVVVVVVVVVVVVVVVVVVVV\n",
    "    ('ohe',    OneHotEncoder(drop=None, sparse=False, handle_unknown='ignore'), categorical_features + ordinal_features_oth)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_preprocessor = ColumnTransformer(transformers=feature_transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_preprocessor.fit(df_train_imp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = numeric_features + ordinal_features_reg + list(feature_preprocessor.named_transformers_['ohe'].get_feature_names(categorical_features + ordinal_features_oth))\n",
    "new_columns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imp_encode = feature_preprocessor.transform(df_train_imp)\n",
    "X_valid_imp_encode = feature_preprocessor.transform(df_valid_imp)\n",
    "X_test_imp_encode  = feature_preprocessor.transform(df_test_imp)\n",
    "\n",
    "df_train_imp_encode = pd.DataFrame(X_train_imp_encode, index=df_train_imp.index, columns=new_columns)\n",
    "df_valid_imp_encode = pd.DataFrame(X_valid_imp_encode, index=df_valid_imp.index, columns=new_columns)\n",
    "df_test_imp_encode  = pd.DataFrame(X_test_imp_encode,  index=df_test_imp.index,  columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train[\"SalePrice\"]\n",
    "y_valid = df_valid[\"SalePrice\"]\n",
    "y_test  = df_test[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imp_encode.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: `DummyRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyRegressor()\n",
    "dummy.fit(X_train_imp_encode, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.score(X_train_imp_encode, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.score(X_valid_imp_encode, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, a negative score??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression intro (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression is one of the most basic and popular ML/statistical techniques. \n",
    "- We already saw logistic regression for classification - these are close cousins.\n",
    "- Both are very _interpretable_.\n",
    "- For more depth, see CPSC 340, STAT 306, and various other STAT courses.\n",
    "- In scikit-learn, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I am going to recommend always using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Ridge` is more flexible than `LinearRegression` because it has a hyperparameter `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_vanilla = LinearRegression()\n",
    "lr_vanilla.fit(X_train_imp_encode, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_vanilla_preds = lr_vanilla.predict(X_valid_imp_encode)\n",
    "lr_vanilla_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_vanilla_preds.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_vanilla_preds.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One prediction is for $\\$10^{15}$ !!\n",
    "- One prediction is for $ - \\$10^{15}$ !!!!\n",
    "- This happened because we have \"collinear features\" and there are \"numerical issues\" (see also CPSC 302).\n",
    "- `Ridge` \"solves\" this issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge()\n",
    "lr.fit(X_train_imp_encode, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(X_valid_imp_encode)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Ridge` has a hyperparameter `alpha`.\n",
    "- This is like `C` in `LogisticRegression` but, annoyingly, `alpha` is the _inverse_ of `C`.\n",
    "  - That is, large `C` is like small `alpha` and vice versa.\n",
    "- We can (approximately) recover the original `LinearRegression` by setting `alpha=0` (but I don't recommend this).\n",
    "- TL;DR don't use `LinearRegression`\n",
    "  - Unless you know what you're doing, but then use [statsmodels](https://www.statsmodels.org/stable/index.html) or R instead of sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with the random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge()\n",
    "lr.fit(X_train_imp_encode, y_train_log);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MAPE: %.1f%%\" % mape(y_train, np.exp(lr.predict(X_train_imp_encode))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation MAPE: %.1f%%\" % mape(y_valid, np.exp(lr.predict(X_valid_imp_encode))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=111)\n",
    "rf.fit(X_train_imp_encode, np.log(y_train));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MAPE: %.1f%%\" % mape(y_train, np.exp(rf.predict(X_train_imp_encode))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation MAPE: %.1f%%\" % mape(y_valid, np.exp(rf.predict(X_valid_imp_encode))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far, the linear model seems to be doing better. \n",
    "- Note that it's also a _simpler_ model.\n",
    "- Like logistic regression, the intuition is that \"the complexity grows as you add more features\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression score functions: mean squared error and R^2 (10 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aren't doing classification anymore, so we can't just check for equality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.predict(X_train_imp_encode) == y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.predict(X_train_imp_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need a score that reflect how right/wrong each prediction is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean squared error (MSE)\n",
    "\n",
    "A common measure is mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dummy.predict(X_train_imp_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((y_train - preds)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also implemented in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_train, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perfect predictions would have MSE=0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But is the above score good or bad? \n",
    "  - It depends on the scale of the targets.\n",
    "  - If we were working in cents instead of dollars, our MSE would be 10,000x ($100^2$) higher!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((y_train*100 - preds*100)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A common score is the $R^2$. \n",
    "- You can [read about it](https://en.wikipedia.org/wiki/Coefficient_of_determination) if interested.\n",
    "- Intuition: mean squared error, but flipped (higher is better), and normalized so the max is 1.\n",
    "- Key points:\n",
    "  - The maximum is 1 for perfect predictions\n",
    "  - Negative values are very bad: \"worse than `DummyRegressor`\" (very bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_valid, dummy.predict(X_valid_imp_encode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the score that sklearn uses by default when you call `score()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.score(X_train_imp_encode, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.score(X_valid_imp_encode, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) Warning: MSE is \"reversible\" but $R^2$ is not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you call `fit` it minimizes MSE / maximizes $R^2$ by default.\n",
    "- Just like in classification, this isn't always what you want!!\n",
    "- More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring `alpha` (5 min)\n",
    "\n",
    "- The `alpha` hyperparameter controls the fundamental tradeoff as usual.\n",
    "  - Smaller `alpha`: lower training error.\n",
    "  - Larger `alpha`: lower approximation error (hopefully). \n",
    "- General intuition: larger `alpha` leads to **smaller coefficients**. \n",
    "  - Smaller coefficients mean the predictions are less sensitive to changes in the data.\n",
    "  - Hence less chance of overfitting (seeing big dependencies when you shouldn't).\n",
    "- Let's test this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=0.01)\n",
    "lr.fit(X_train_imp_encode, y_train_log);\n",
    "np.max(np.abs(lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=1)\n",
    "lr.fit(X_train_imp_encode, y_train_log);\n",
    "np.max(np.abs(lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=1000)\n",
    "lr.fit(X_train_imp_encode, y_train_log);\n",
    "np.max(np.abs(lr.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that it will not make all the coefficients smaller in the same proportion!\n",
    "- The order of the coefficients might change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=0.01)\n",
    "lr.fit(X_train_imp_encode, y_train_log);\n",
    "df_train_imp_encode.columns[np.argmax(np.abs(lr.coef_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=1)\n",
    "lr.fit(X_train_imp_encode, y_train_log);\n",
    "df_train_imp_encode.columns[np.argmax(np.abs(lr.coef_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=100)\n",
    "lr.fit(X_train_imp_encode, y_train_log);\n",
    "df_train_imp_encode.columns[np.argmax(np.abs(lr.coef_))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try tuning `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10.0**np.arange(-1.5,4.5,0.5)\n",
    "train_errs = []\n",
    "valid_errs = []\n",
    "for alpha in alphas:\n",
    "\n",
    "    lr = Ridge(alpha=alpha)\n",
    "    lr.fit(X_train_imp_encode, y_train_log);\n",
    "    train_errs.append(mape(y_train, np.exp(lr.predict(X_train_imp_encode))))\n",
    "    valid_errs.append(mape(y_valid, np.exp(lr.predict(X_valid_imp_encode))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(alphas, train_errs, label=\"train\");\n",
    "plt.semilogx(alphas, valid_errs, label=\"valid\");\n",
    "plt.legend();\n",
    "plt.xlabel('alpha');\n",
    "plt.ylabel('MAPE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alphas[np.argmin(valid_errs)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errs = []\n",
    "valid_errs = []\n",
    "for alpha in alphas:\n",
    "\n",
    "    lr = Ridge(alpha=alpha)\n",
    "    lr.fit(X_train_imp_encode, y_train);\n",
    "    train_errs.append(np.sqrt(mean_squared_error(y_train, lr.predict(X_train_imp_encode))))\n",
    "    valid_errs.append(np.sqrt(mean_squared_error(y_valid, lr.predict(X_valid_imp_encode))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(alphas, train_errs, label=\"train\");\n",
    "plt.semilogx(alphas, valid_errs, label=\"valid\");\n",
    "plt.legend();\n",
    "plt.xlabel('alpha');\n",
    "plt.ylabel('MSE');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alphas[np.argmin(valid_errs)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These are interesting curves, because the validation error is less than the training error.\n",
    "- In short, that is because when `alpha` is large we are no longer directly minimizing training error.\n",
    "- It seems `alpha=100` is the best choice here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(alpha=100)\n",
    "lr.fit(X_train_imp_encode, y_train_log);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MAPE: %.1f%%\" % mape(y_train, np.exp(lr.predict(X_train_imp_encode))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation MAPE: %.1f%%\" % mape(y_valid, np.exp(lr.predict(X_valid_imp_encode))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm sure one could do a lot better on this dataset, but 9% MAPE is a start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the targets (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something more serious than `DummyRegressor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=111)\n",
    "rf.fit(X_train_imp_encode, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_train_imp_encode, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_valid_imp_encode, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_valid = mean_squared_error(y_valid, rf.predict(X_valid_imp_encode))\n",
    "mse_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case, we are actually interested in the original units of dollars.\n",
    "- MSE is in units of \"squared dollars\"\n",
    "- We can take the square root of the this to get back to dollars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_valid = np.sqrt(mse_valid)\n",
    "rmse_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, on average we're off by about \\$25000. Is this good?\n",
    "\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "- For a house worth \\\\$500k, it seems reasonable! That's 5% error.\n",
    "- For a house worth \\\\$50k, that is terrible. It's 50% error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indeed, we have both of these cases in our dataset.\n",
    "- Can we compute percent error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = rf.predict(X_train_imp_encode)\n",
    "pred_valid = rf.predict(X_valid_imp_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_errors = (pred_train - y_train)/y_train\n",
    "percent_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These are both positive (predict too high) and negative (predict too low).\n",
    "- We can look at the absolute percent error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(percent_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, like MSE, we can take the average over examples. This is called **mean absolute percent error (MAPE)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(true, pred):\n",
    "    return 100.*np.mean(np.abs((pred - true)/true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_valid, pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ok, this is quite interpretable.\n",
    "- On average, we have 11% error. Good to know. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ... but wait a minute, why are we minimizing MSE if we care about MAPE??\n",
    "- When minimizing MSE, **the expensive houses will dominate** because they have the biggest error.\n",
    "- The model would rather do \\\\$50 k worse on a cheap place and \\\\$60k better on an expensive place.\n",
    "  - But this would make the MAPE much worse!\n",
    "- Key idea: **log transform the targets**.\n",
    "  - That is, transform $y\\rightarrow \\log(y)$.\n",
    "- Why?\n",
    "- Let's assume we have two values, the prediction and the true value.\n",
    "- We log transform them and look at the squared error:\n",
    "\n",
    "$$\\begin{align}(\\log y_\\text{pred} - \\log y_\\text{true})^2 =\\left(\\log \\frac{y_\\text{pred}}{y_\\text{true}}\\right)^2  \\end{align}$$\n",
    "\n",
    "But what is the absolute percent error? It is\n",
    "\n",
    "$$\\left| \\frac{y_\\text{pred} - y_\\text{true}}{y_\\text{true}} \\right| = \\left| \\frac{y_\\text{pred}}{y_\\text{true}} -1 \\right|$$\n",
    "\n",
    "This is a bit hand-wavy (and maybe I'll nail this down later), but for now: both are minimized (equal to zero) when $\\frac{y_\\text{pred}}{y_\\text{true}}=1$, so both are trying to make $\\frac{y_\\text{pred}}{y_\\text{true}}$ close to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(y_train), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_log = RandomForestRegressor(random_state=111)\n",
    "rf_log.fit(X_train_imp_encode, np.log(y_train));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_log.predict(X_train_imp_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are log predictions. We can `exp` them to get back to dollars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_log_exp = np.exp(rf_log.predict(X_train_imp_encode))\n",
    "pred_valid_log_exp = np.exp(rf_log.predict(X_valid_imp_encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_log_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the MSE on the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(pred_train, y_train) # ORIGINAL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(pred_train_log_exp, y_train) # log transformed training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MSE got worse\n",
    "  - That makes sense because we're no longer optimizing for MSE.\n",
    "- Let's look at the MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_train, pred_train_log_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MAPE got better!\n",
    "- We can do the same for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_valid, pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_valid, pred_valid_log_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we get a small benefit, but it could be very large in some cases.\n",
    "- Also, the model's interpretation can often be better in the log transformed case.\n",
    "- For linear regression: \n",
    "  - 1 more bedroom increases price by \\\\$50K vs\n",
    "  - 1 more bedroom increases price by 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Note that this assumes the $y$-values are positive, which is true in this case.\n",
    "  - There is still a problem if one of the $y$-values is zero, so it's common to do $\\log(1+y)$ instead of $\\log(y)$.\n",
    "  - There is even a numpy function to do this for you: [`log1p`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log1p.html) (better handing of floating point issues for small numbers -- not too relevant to us here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling with Regression (5 min)\n",
    "mention how ensembling works with regression\n",
    "\n",
    "1. show that it's the actual average\n",
    "2. show how the LR coefs are now Ridge coefs.\n",
    "3. Stacking doesn't have to use predict_proba - well, there is no predict_proba!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression True/False (Piazza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If the first coefficient is 5, that means increasing your first feature by 1 increases the prediction by 5.\n",
    "2. Since the `PoolArea` has a positive coefficient, expanding my pool will get me a higher price when I sell my house.\n",
    "3. Larger values of `alpha` are probably more useful when I have lots of features.\n",
    "4. log-transforming the targets (and re-fitting) is equivalent to log-transforming the coefficients. \n",
    "5. In regression, one should use MAPE instead of MSE when relative (percent) error matters more than absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
