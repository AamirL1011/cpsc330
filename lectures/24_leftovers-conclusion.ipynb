{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 Lecture 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "\n",
    "- Announcements\n",
    "- Gradient boosted trees, `Catboost` (15 min)\n",
    "- Model comparison (15 min)\n",
    "- Break (5 min)\n",
    "- Big data sets: `SGDClassifier` and `SGDRegressor` (10 min)\n",
    "- Combining multiple tables (10 min)\n",
    "- True/False questions (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder to self: **turn on recording!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge, LogisticRegression, SGDClassifier, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These next 3 cells could just be replaced with `import load_process_data`.\n",
    "- But this way I can make changes to that file and not need to restart my kernel each time.\n",
    "- So it's convenient for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'load_process_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e279bf576bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aimport'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'load_process_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/envs/cpsc330env/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-873>\u001b[0m in \u001b[0;36maimport\u001b[0;34m(self, parameter_s, stream)\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cpsc330env/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cpsc330env/lib/python3.8/site-packages/IPython/extensions/autoreload.py\u001b[0m in \u001b[0;36maimport\u001b[0;34m(self, parameter_s, stream)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                 \u001b[0mtop_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0;31m# Inject module to user namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cpsc330env/lib/python3.8/site-packages/IPython/extensions/autoreload.py\u001b[0m in \u001b[0;36maimport_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_module_reloadable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mtop_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mtop_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cpsc330env/lib/python3.8/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cpsc330env/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cpsc330env/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/cpsc330env/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'load_process_data'"
     ]
    }
   ],
   "source": [
    "%aimport load_process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_classifier import plot_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "\n",
    "- this year we already introduced these in lecture 9\n",
    "- but we still shoud talk about how CatBoost handles categoricals? And some other details below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_scores_factory(X_train, y_train, X_test, y_test):\n",
    "    def show_scores(model, **fit_kwargs):\n",
    "        model.fit(X_train, y_train, **fit_kwargs);\n",
    "        return model.score(X_test, y_test)\n",
    "    return show_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Announcements\n",
    "\n",
    "Same as last class:\n",
    "\n",
    "- hw7 has been posted, due Sunday evening.\n",
    "  - I tried to make it shorter than previous assignments.\n",
    "- Tutorials are still happening as scheduled, on Collaborate Ultra.\n",
    "- Change to course grading scheme per Dean's directive; see [here](https://piazza.com/class/k1gx4b3djbv3ph?cid=319)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's class:\n",
    "\n",
    "- A bunch of random things I wanted to cover at some point.\n",
    "- Next week: communication and ethics (hopefully)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted trees (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently a lot of the winning models have been one of the following:\n",
    "\n",
    "|  Name   |  GitHub |  web/docs | Year | GitHub stars |\n",
    "|---------|---------|---------|---------|--------------|\n",
    "|  XGBoost | [link](https://github.com/dmlc/xgboost) | [link](https://xgboost.ai/) | 2016 | 19k\n",
    "| LightGBM |  [link](https://github.com/microsoft/LightGBM) | [link](https://lightgbm.readthedocs.io/en/latest/) |  2017 | 11k |\n",
    "| CatBoost | [link](https://github.com/catboost/catboost) | [link](https://catboost.ai/) | 2017 | 5k |\n",
    "\n",
    "When I checked, all repos updated in the last 24 hrs (i.e., active development)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All of these implement [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting), which is beyond the scope of the course.\n",
    "  - And not covered in 340, but it probably should be!\n",
    "- These 3 packages are fairly similar. Today I'll focus on CatBoost.\n",
    "- I believe CatBoost and LightGBM seems to be performing better than XGBoost these days, but they are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I avoided these earlier in the course because I thought they'd be hard to install.\n",
    "- It seems a lot of issues were solved since I tried XGBoost 1-2 years ago - it was easy!\n",
    "- I think next time I might put these a lot earlier and use CatBoost instead of sklearn random forests everywhere, because I think it's a better choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical variables\n",
    "\n",
    "- Of these 3, CatBoost is special because of how it handles categorical variables - hence \"Cat\".\n",
    "- Thinking back to decision trees, we talked about splits like `Humidity > 1.2` or `Longitude > 90`. \n",
    "- Because we OHE categorical variables, it will give splits like `Location_Sydney > 0.5`, where that number 0.5 could be anywhere between 0 and 1. \n",
    "- (First off, there is no reason sklearn's decision trees or random forests need to require OHE; they could have easily learned rules like `Location == 'Sydney'`, which is equivalent to `Location_Sydney > 0.5` after OHE. I believe they didn't do this to be consistent with other sklearn estimators. However, I wonder if this makes performance worse, in particular for random forests.)\n",
    "- However, is OHE the best encoding? Do we want 100 columns if there are 100 categories? What about `drop='first'` and `handle_error='ignore'`. It seems like a lot of hassle.\n",
    "- CatBoost uses a more sophisticated encoding from categorical to numeric; see [here](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html#algorithm-main-stages_cat-to-numberic).\n",
    "  - From the documentation: \"Before each split is selected in the tree (see Choosing the tree structure), categorical features are transformed to numerical. This is done using various statistics on combinations of categorical features and combinations of categorical and numerical features.\"\n",
    "  - This makes me think the conversion is done separately _at each split_.\n",
    "  - So we can't just do this transformation in advance, it is entangled deeply with the algorithm. \n",
    "  - And also this makes sense, since the popularity of each value will change as your split the data.\n",
    "  - Importantly, if there are 100 possible categories, you may not end up with 100 columns.\n",
    "  - (From my reading of the documentation it looks like you end up with some smaller number of columns, depending on the type of _targets_: one per class for classification, and for regression this is controlled by a hyperparameter $k$.)\n",
    "  - (Interestingly, the number of columns does not seem to depend on the number of possible categories of the categorical variable!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_housing, X_valid_housing, X_test_housing, y_train_housing, y_valid_housing, y_test_housing = load_process_data.load_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_housing_log = np.log(y_train_housing)\n",
    "y_valid_housing_log = np.log(y_valid_housing)\n",
    "y_test_housing_log  = np.log(y_test_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_housing = show_scores_factory(X_train_housing, y_train_housing_log, X_valid_housing, y_valid_housing_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_housing(Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_housing(RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_housing(xgb.XGBRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_housing(lgb.LGBMRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_housing(CatBoostRegressor(), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we try with proper handling of categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_housing_cat, X_valid_housing_cat, X_test_housing_cat, y_train_housing, y_valid_housing, y_test_housing, categorical_features = load_process_data.load_housing(ohe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_housing_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_housing_cat[categorical_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostRegressor(cat_features=categorical_features)\n",
    "cat.fit(X_train_housing_cat, y_train_housing_log, verbose=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.score(X_valid_housing_cat, y_valid_housing_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the code ran slower and it didn't seem to help, but I suspect it will in some cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The syntax for getting feature importances is a bit clunky.\n",
    "- You first need to create a `catboost.Pool` object storing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pool = Pool(X_train_housing_cat, y_train_housing_log, cat_features=categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you can get feature importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = cat.get_feature_importance(train_data_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can display them as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame(data=importances, index=X_train_housing_cat.columns, columns=[\"Importance\"])\n",
    "importances.sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also get SHAP values, both through the SHAP package or built-in to CatBoost.\n",
    "- With proper handling of categorical variables, we can see an importance for the entire categorical variable, not just one possible value of it, which is kind of nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very rough rule of thumb, when working on a problem with **tabular data**, try the following, in order: \n",
    "\n",
    "1. `DummyRegressor` or `DummyClassifer`\n",
    "2. `HuberRegressor` or `LogisticRegression`\n",
    "3. `CatBoostRegressor` or `CatBoostClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now do a \"bake-off\" between different models and different datasets in the course, following the outlier scavenger hunt from L19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Census data (lecture 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_adult, y_train_adult, X_test_adult, y_test_adult, _ = load_process_data.load_adult()\n",
    "X_train_adult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_adult = show_scores_factory(X_train_adult, y_train_adult, X_test_adult, y_test_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_adult(DummyClassifier(strategy=\"prior\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_adult(LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_adult(xgb.XGBClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_adult(lgb.LGBMClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_adult(CatBoostClassifier(), verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_adult_cat, y_train_adult, X_test_adult_cat, y_test_adult, cat_features_adult = load_process_data.load_adult(ohe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_adult_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostClassifier(cat_features=cat_features_adult)\n",
    "cat.fit(X_train_adult_cat, y_train_adult, verbose=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.score(X_test_adult_cat, y_test_adult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "compare on AUC as well, accuracy might not be as meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie review data (lecture 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imdb, y_train_imdb, X_test_imdb, y_test_imdb = load_process_data.load_movie()\n",
    "X_train_imdb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_movie = show_scores_factory(X_train_imdb, y_train_imdb, X_test_imdb, y_test_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_movie(DummyClassifier(strategy=\"prior\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_movie(LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_movie(xgb.XGBClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM seems to kill my kernel in this case... too many features?? Or maybe it can't handle sparse matrices as input? That seems surprising though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_scores_movie(lgb.LGBMClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_movie(CatBoostClassifier(), verbose=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no categorical features here (they are all word counts), so no need to do another experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rain in Australia data (lecture 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rain, y_train_rain, X_test_rain, y_test_rain, _ = load_process_data.load_rain()\n",
    "X_train_rain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_rain = show_scores_factory(X_train_rain, y_train_rain, X_test_rain, y_test_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_rain(DummyClassifier(strategy=\"prior\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_rain(LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_rain(xgb.XGBClassifier(verbosity=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_rain(lgb.LGBMClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores_rain(CatBoostClassifier(), verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rain_cat, _, X_test_rain_cat, _, cat_features_rain = load_process_data.load_rain(ohe=False)\n",
    "X_train_rain_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rain_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostClassifier(iterations=500, cat_features=cat_features_rain)\n",
    "cat.fit(X_train_rain_cat, y_train_rain, verbose=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.score(X_test_rain_cat, y_test_rain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "- Overall CatBoost looks like a good bet. \n",
    "- We didn't do any hyperparameter tuning, this might change the results. Maybe they just have better defaults.\n",
    "  - In fact, we did not discuss hyperparameters at all for these fancy models.\n",
    "  - There are a ton of them!\n",
    "  - For CatBoost, a main hyperparameter controlling the fundamental tradeoff is `iterations`. \n",
    "    - This is similar to `n_estimators` in the sklearn random forests: larger = more complex models.\n",
    "  - Another main hyperparameter is `learning_rate` but its interpretation is a bit beyond our scope.\n",
    "  - These models should all be compatible with `RandomizedSearchCV`, etc.\n",
    "  - But CatBoost, at least, does not work out of the box for feature selection with `RFE`/`RFECV`\n",
    "- But I think it's pretty good. A bit slow though.\n",
    "  - Now that speed is an issue, we should note that increasing complexity via `iterations` also increases runtime.\n",
    "  - This is not always true, e.g. `gamma` in SVM and (for the most part) `C` in `LogisticRegression`.\n",
    "  - But in fact often we did get increased runtime with more complexity, e.g. more features from `CountVectorizer`. \n",
    "- I'm surprising that feeding in the raw categorical features doesn't seem to help in many cases. \n",
    "  - Isn't that the point of CatBoost?!\n",
    "  - It does seem to help with the Rain data.\n",
    "  - I will need to look into this more at some point.\n",
    "    - Maybe the code needs to be run for longer in these cases?\n",
    "    - Hyperparameter tuning should be done in each case, at least a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big data sets: `SGDClassifier` and `SGDRegressor` (10 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Something we haven't discussed in this course is huge data sets.\n",
    "- There could be a couple problems arising from huge data sets:\n",
    "\n",
    "1. The code is too slow.\n",
    "2. The dataset doesn't fit in memory - I can't even load it with `pd.read_csv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"SGD\" (stochastic gradient descent) can help with both of these problems.\n",
    "- But we'll focus on using it to solve problem (1).\n",
    "- There is a fancy way to implement `fit` that can be a lot faster for big data sets.\n",
    "  - You can think of it as quickly finding \"approximately\" the best coefficients when calling `fit`.\n",
    "  - That is not quite true but it may be a useful way of thinking.\n",
    "  - Much more on this in CPSC 340 and much, much more on this in CPSC 440/540."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD can be used in many contexts.\n",
    "- In sklearn, it's built in as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `SGDRegressor` is basically equivalent to `Ridge`.\n",
    "- `SGDRegressor(loss='huber')` is basically equivalent to `HuberRegressor`.\n",
    "- `SGDClassifier(loss='log')` is basically equivalent to `LogisticRegression`, except the parameter is called `alpha` instead of `C` (like `Ridge`).\n",
    "- With other settings they are equivalent to other models, but this is good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For regular sized datasets, use the original functions, as these ones can be a bit more finicky. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the [Sentiment140 dataset](http://help.sentiment140.com/home), which contains tweets labeled with sentiment associated with a brand, product, or topic. (I don't think we've looked at this dataset before - using it here because it's large.) You can download the data from [here](https://www.kaggle.com/ferno2/training1600000processednoemoticoncsv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('data/training.1600000.processed.noemoticon.csv', \n",
    "                        encoding = \"ISO-8859-1\",\n",
    "                        names=[\"label\",\"id\", \"date\", \"no_query\", \"name\", \"text\"])\n",
    "tweets_df['label'] = tweets_df['label'].map({0: 'neg', 4: 'pos'})\n",
    "tweets_df = tweets_df[tweets_df['label'].str.startswith(('pos','neg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_train, tweets_df_test = train_test_split(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>no_query</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>794225</th>\n",
       "      <td>neg</td>\n",
       "      <td>2326914448</td>\n",
       "      <td>Thu Jun 25 07:40:39 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>richardchisholm</td>\n",
       "      <td>@Spacefrog29  alright then, im clean i swear! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292094</th>\n",
       "      <td>neg</td>\n",
       "      <td>1995895449</td>\n",
       "      <td>Mon Jun 01 14:12:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jolizevette</td>\n",
       "      <td>@strawintogold I had class this morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500684</th>\n",
       "      <td>neg</td>\n",
       "      <td>2186927863</td>\n",
       "      <td>Mon Jun 15 19:32:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_AlexaJordan</td>\n",
       "      <td>@Ryan_ADT Yea totally~ Just can't make it toni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517786</th>\n",
       "      <td>neg</td>\n",
       "      <td>2191384005</td>\n",
       "      <td>Tue Jun 16 05:09:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Iam_Angela</td>\n",
       "      <td>Where is everybody this morning? I've got not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470474</th>\n",
       "      <td>neg</td>\n",
       "      <td>2176430433</td>\n",
       "      <td>Mon Jun 15 03:51:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mileythomson</td>\n",
       "      <td>leaving for college soon. i have a communicati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label          id                          date  no_query  \\\n",
       "794225   neg  2326914448  Thu Jun 25 07:40:39 PDT 2009  NO_QUERY   \n",
       "292094   neg  1995895449  Mon Jun 01 14:12:55 PDT 2009  NO_QUERY   \n",
       "500684   neg  2186927863  Mon Jun 15 19:32:22 PDT 2009  NO_QUERY   \n",
       "517786   neg  2191384005  Tue Jun 16 05:09:40 PDT 2009  NO_QUERY   \n",
       "470474   neg  2176430433  Mon Jun 15 03:51:16 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   name                                               text  \n",
       "794225  richardchisholm  @Spacefrog29  alright then, im clean i swear! ...  \n",
       "292094      jolizevette           @strawintogold I had class this morning   \n",
       "500684     _AlexaJordan  @Ryan_ADT Yea totally~ Just can't make it toni...  \n",
       "517786       Iam_Angela  Where is everybody this morning? I've got not ...  \n",
       "470474     mileythomson  leaving for college soon. i have a communicati...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200000, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy cow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train = vec.fit_transform(tweets_df_train['text']) \n",
    "y_train = tweets_df_train['label']\n",
    "\n",
    "X_test = vec.transform(tweets_df_test['text']) \n",
    "y_test = tweets_df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the fraction of elements that are nonzero. Having a sparse matrix really helps!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.nnz/np.prod(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a classifier. I'll use `time` instead of `%timeit` because I want to keep the output, and it gets lost with `%timeit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sgd = SGDClassifier(loss=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr_sgd.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sgd.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sgd.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `SGDClassifier` was about 10x faster than `LogisticRegression`, but the accuracy is slightly lower.\n",
    "- In fact, we can control the speed vs. _train_ accuracy tradeoff in both cases using the hyperparameters.\n",
    "  - The main ones are `max_iter` (higher is slower) and/or `tol` (lower is slower)\n",
    "  - (This is the same for both `LogisticRegression` and `SGDClassifier`)\n",
    "- In general, `LogisticRegression` will get slightly higher _train_ accuracy (may or may not correspond to better validation/test)\n",
    "- But in some cases your dataset is so big that `LogisticRegression` is not feasible, and then `SGDClassifier` can save the day.\n",
    "\n",
    "Random comment: last time I tried this was with sklearn 0.21.3 and scipy 1.3.1 a few months ago, now with skearln 0.22.1 and scipy 1.4.1. I wonder if `LogisticRegression`, or the underlying optimizer in scipy, was improved at all in these recent upgrades, because it seems to be faster now than last time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining multiple tables (10 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take a look at the [Formula 1 race data set](https://www.kaggle.com/cjgdev/formula-1-race-data-19502017) from Kaggle. \n",
    "- The dataset contains **multiple CSV files**.\n",
    "- Let's read in one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racing_results_df = pd.read_csv(\"data/formula-1-race-data-19502017/results.csv\", index_col=0)\n",
    "racing_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's say we want to predict the `milliseconds` column, namely the total length of time it takes a driver to finish a race. \n",
    "- In that case, we should not have access to most of these other columns. \n",
    "- But we would have the `raceId` and `driverId`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racing_results_df_subset = racing_results_df[['raceId', 'driverId', 'milliseconds']]\n",
    "racing_results_df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racing_results_df_subset.sort_values(by=\"driverId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "for fun, try to predict milliseconds just based on raceId and driverId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we need some features to predict the race time. \n",
    "- Enter the other tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racing_drivers_df = pd.read_csv(\"data/formula-1-race-data-19502017/drivers.csv\", \n",
    "                                encoding='latin-1', index_col=0,\n",
    "                               parse_dates=['dob'])\n",
    "racing_drivers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can we use the driver's nationality and age as features?\n",
    "- `pd.merge` can take care of this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(racing_results_df_subset, racing_drivers_df, on=\"driverId\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `on` keyword told it which column to use to match up the rows of the two dataframes.\n",
    "- Note that the first 5 rows have the same `driverId`, so they pulled the same data from `racing_drivers_df`.\n",
    "- Now we could keep only the columns we plan to encode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_subset = merged_df[['raceId', 'driverId', 'milliseconds', 'dob', 'nationality']]\n",
    "merged_df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process the `dob` column to get age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = (pd.Timestamp.now() - merged_df_subset[\"dob\"]).apply(lambda x: x.total_seconds()/3600/24/365)\n",
    "merged_df_age = merged_df_subset.assign(age=ages)\n",
    "merged_df_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far we got information for each driver.\n",
    "- Likewise, we can get information about the races, and use those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racing_races_df = pd.read_csv(\"data/formula-1-race-data-19502017/races.csv\", encoding='latin-1', index_col=0)\n",
    "racing_races_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For those who have taken CPSC 304 or some have other database training, you'll recognize this type of multi-table situation, with foreign keys connecting the tables.\n",
    "- `pd.merge` supports several types of joins, see the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True/False questions (15 min)\n",
    "\n",
    "1. `CatBoost` is effective because, when increasing `iterations`, you lower the training error and the approximation error. \n",
    "2. `CatBoost` is likely to be popular in 10 years.\n",
    "3. The primary motivation for using `SGDClassifier` or `SGDRegressor` is speed.\n",
    "4. In multi-class logistic regression, if the coefficient for feature 10, class 2 is positive, that means increasing the value of feature 10 _decreases_ the predicted probability of class 1 (a different class).\n",
    "5. If we are dealing with data from multiple sources, our strategy is to first combine them as a preprocessing step, and then build a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course review (15 min)\n",
    "\n",
    "#### Learning objectives\n",
    "\n",
    "Here are the course learning outcomes I came up with when proposing this new course:\n",
    "\n",
    "1. Identify problems that may be addressed with machine learning.\n",
    "2. Select the appropriate machine learning tool for a problem.\n",
    "3. Transform data of various types into usable features.\n",
    "4. Apply standard tools implementing supervised and unsupervised learning techniques.\n",
    "5. Describe core differences between training, validation, and testing regimes.\n",
    "6. Effectively communicate the results of a machine learning pipeline.\n",
    "7. Be realistic about the limitations of individual approaches and machine learning as a whole. \n",
    "8. Create reproducible workflows and pipelines.\n",
    "\n",
    "- How did we do? \n",
    "- Hopefully OK, except we skipped the last point (that will likely be its own new course).\n",
    "- I would also add:\n",
    "\n",
    "9. Identify and avoid scenarios in which training and testing data are accidentally mixed (the \"Golden Rule\").\n",
    "10. Employ good habits for applying ML, such as starting an analysis with a baseline estimator.\n",
    "\n",
    "because I think they are important enough to make it to the course-level list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What did we cover?\n",
    "\n",
    "I see the course roughly like this (not in order):\n",
    "\n",
    "Part 1: Supervised learning on tabular data\n",
    "\n",
    "- Overfitting, train/validation/test/deployment, cross-validation\n",
    "- Feature preprocessing, pipelines, imputation, OHE, etc\n",
    "- The Golden Rule, various ways to accidentally violate it\n",
    "- Classification metrics: confusion matrix, precision/recall, ROC, AUC\n",
    "- Regression metrics: MSE, MAPE\n",
    "- Regression: transforming the targets\n",
    "- Feature importances, feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Other data types (non-tabular)\n",
    "\n",
    "- Time series\n",
    "- Right-censored data / survival analysis\n",
    "- Computer vision with deep learning\n",
    "- Language data, text preprocessing\n",
    "- Ratings data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Other stuff\n",
    "\n",
    "- Some Python (numpy, pandas, scipy sparse matrices)\n",
    "- Hyperparameter optimization\n",
    "- Ensembles\n",
    "- Outlier detection\n",
    "- Clustering\n",
    "- A bunch of models: \n",
    "  - Dummy*\n",
    "  - linear models (ridge, lasso, huber, logistic regression, SGD*)\n",
    "  - tree-based models (random forest, gradient boosted trees)\n",
    "  - KNN classifier/regressor\n",
    "  - pre-trained deep learning models\n",
    "- Communicating your results (including visualizations)\n",
    "- ML skepticism\n",
    "- Ethics for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What would I do differently?\n",
    "\n",
    "Lots of things, of course! Here are some important ones:\n",
    "\n",
    "- Introduce `Pipeline` earlier.\n",
    "- Throughout the course, default to cross-validation instead of train/valid split.\n",
    "- Find a dataset with multi-class classification for a section of the course.\n",
    "- Spend more time on quantifying the uncertainty in one's results (scores, predictions, feature importances, etc).\n",
    "- Add a lecture on deploying a trained model.\n",
    "- Skip some of the content on text preprocessing.\n",
    "- Skip some of the content on SVMs.\n",
    "- Skip `Lasso`?\n",
    "\n",
    "I'm sure you have other suggestions - feel free to drop me an email, submit my contact form anonymously at mikegelbart.com, or drop them in the course evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 330 vs. 340\n",
    "\n",
    "- Just talked about it - see recording.\n",
    "\n",
    "# TODO - add this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What was not covered\n",
    "\n",
    "- Deployment\n",
    "- Big data, distributed computing\n",
    "- How ML methods work (CPSC 340)\n",
    "- Probabilistic methods\n",
    "- A lot of unsupervised learning, semi-supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsolicited advice: working with others (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I sometimes end my courses with \"unsolicited life advice\".\n",
    "- I won't repeat myself here because some of you took CPSC 340 with me. But if you're interested [it's on YouTube](https://www.youtube.com/watch?v=_7zYxpzrKmQ&list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b&index=34&t=0s).\n",
    "- Instead of general life advice I'll try a different topic this time: unsolicited advice on _working with others_.\n",
    "- These are just my opinions. They not be complete, or correct. Follow my advice at your own risk!\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't lead with blame - investigate first\n",
    "\n",
    "Leading with blame:\n",
    "\n",
    "> Hey Malcolm, you were supposed to submit this form by the deadline - why didn't you?\n",
    "\n",
    "Instead, try this:\n",
    "\n",
    "> Hey Malcolm, from my end it looks like the form hasn't been submitted - can you shed some light on the situation?\n",
    "\n",
    "- Blaming others is very embarrassing and damaging if the blame is not deserved.\n",
    "- And also not great if the blame _is_ deserved.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fundamental attribution error\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Fundamental_attribution_error\n",
    "- If you miss a deadline: “I was too busy moving apartments.”\n",
    "- If your teammate misses a deadline: “They are incompetent.”\n",
    "- This is a known psychological phenomenon, so try to correct for this. Are you sure you know why they missed the deadline?\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Don't procrastinate on disappointing others\n",
    "\n",
    "- This can be highly damaging, and is a serious form of procrastination.\n",
    "- If you need to break a commitment, communicate this right away. \n",
    " - Can't get your work done on time.\n",
    " - Need to pull out of a project.\n",
    " - Need to move your organization to another city.\n",
    "- Consider how much better this is for the person being disappointed.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your opinion is not special\n",
    "\n",
    "- If you disagree with someone, why do you think you're more likely to be right than the other person?\n",
    "  - After all, there's a symmetry to the situation.\n",
    "- I think most people are in denial about this.\n",
    "  - That is, if you take an issue (e.g. \"will lowering taxes improve the economy?\", or religious beliefs), the credence of opposing sides are likely both above 50%. \n",
    "- A good question to ask yourself: is there data? E.g. if you are always on time and your co-worker is always late, then OK to trust your opinion on scheduling.\n",
    "- My approach:\n",
    "  - For critical decisions: try to \"average\" different opinions, including my own, based on trustworthiness.\n",
    "  - For most decisions: do it my way because life is more fun that way.\n",
    "  \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That's all, folks! Thank you for your active participation and supportive attitude."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
